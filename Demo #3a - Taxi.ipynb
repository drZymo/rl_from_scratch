{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - Taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will look at the [Gymnasium](https://gymnasium.farama.org/) library, which is a continuation of the now abandoned [Gym](https://github.com/openai/gym) library that was originally created by OpenAI.\n",
    "\n",
    "The library contains a lot of different reinforcement learning environments that you can use to evaluate your algorithms. The environments range from simple environments with only a few states, to complete video games ([Atari](https://gymnasium.farama.org/environments/atari/)) and 3D simulated robots ([MuJoCo](https://gymnasium.farama.org/environments/mujoco/)). All these environments use the same basic interface, which makes them very useful to compare and evaluate RL algorithms, making it the de-facto benchmark in the RL industry.\n",
    "\n",
    "In this demo we will use the `Taxi` environment and we will first explore how it works.\n",
    "\n",
    "Let's create the environment first by simply calling `gymnasium.make`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "env = gymnasium.make('Taxi-v3', render_mode='rgb_array')\n",
    "\n",
    "print(f'Observation: {env.observation_space}, {env.observation_space.dtype}')\n",
    "print(f'Action:      {env.action_space}, {env.action_space.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see that the environment will return observations (a.k.a. state) with a shape `Discrete(500)`. This means that it is a single discrete value (i.e. integer) between 0 and 500. In other environments it can also return `Box()` spaces which will are multidimensional. The action space is also `Discrete`, meaning the environment accepts only a single integer value between 0 and 6 for the action.\n",
    "\n",
    "So this is a very simple environment that gives a single number and accepts a single number.\n",
    "\n",
    "Furthermore, the environment supports rendering the current state so you can see what is going on. In this case it is a nice comic image representation of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_frame, update_frame\n",
    "env.reset()\n",
    "frame = create_frame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world consists of a 5x5 grid with 4 pickup and dropoff positions (R,G,Y,B). There is a Taxi driving around in this world. The passenger is waiting at one of the four colored square and its destination (a hotel) is located on another colored square. There are also some walls to make the route a bit more challenging.\n",
    "\n",
    "The goal of the agent is to pickup the passenger and drop it off at its desired destination as fast as possible. The agent has 6 actions (in this order): moving south (0), north (1), east (2), west (3) and pickup (4) and dropoff (5) of passenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interact with the environment in the following way.\n",
    "\n",
    "First the environment must be reset, so it is initialized and at its start state. This can be achieved with the `env.reset()` function which will return the current state (observation) and extra information that we will not use in this exercise. In Python you can use `_` to discard data. The start state is randomly chosen, so each time you execute the next cell the output should be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the agent can perform an action by calling the `step(a)` function, which only required the action to be taken, which is in this case simply an integer value. It returns the next observation (state) after the action is performed, the resulting reward, a flag indicating if the episode is finished (done), a flag if the episode is truncated (not used now), and some extra information. We will discard the truncated flag and the extra information because they are not used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,reward,done,_,_ = env.step(5)\n",
    "update_frame(frame)\n",
    "reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state the environment returns is a single number, which represent the entire state of the world. There are 5 × 5 = 25 possible states where the taxi can be, there are 5 states where the passenger can be (4 positions and on-board the taxi), and there are 4 destination states. So 25 × 5 × 4 = 500 different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know anything else about this environment. We don't know to which state an action will bring us, what the reward will be, or when an episode is finished. We only have this interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
