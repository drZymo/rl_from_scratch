{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #5a - REINFORCE\n",
    "\n",
    "<img src=\"figures/reinforce.png\" width=\"50%\" align=\"right\"/>\n",
    "\n",
    "In this exercise we will implement the Policy Gradient algorithm REINFORCE, of which the pseudo-code is depicted on the right.\n",
    "\n",
    "This algorithm trains on complete trajectories of an episode to train an estimator that predicts the policy (i.e. action probabilities).\n",
    "\n",
    "First include the basic necesities we are fimiliar with now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "from utils import run_environment, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use the already demonstrated `LunarLander` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('LunarLander-v2', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy estimator\n",
    "\n",
    "For estimating the policy we will use a neural network that predicts action probabilities fpr a given observation. There is not enough time to implement this from scratch in this training, so we will use a given implementation. The class `PolicyEstimator` from the module ['estimator.py'](estimator.py) is implemented in a similar fashion as the `ActionValueEstimator` that we have used before.\n",
    "\n",
    "It has two major functions:\n",
    "- `predict(observation)`: Returns an array of probabilities for each action.\n",
    "- `train(observations, actions, returns)`: Trains the estimator on the given observations. For each observation a corresponding action and a return must be provided that specify which action was taken and what the return of that action was. The probabilities for actions with high returns will be increased more than actions with low returns.\n",
    "\n",
    "If you are interested and if you have time then you could take a look at the [implementation](estimator.py) of this estimator.\n",
    "\n",
    "Let's import it and create a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimator import PolicyEstimator\n",
    "\n",
    "policy_estimator = PolicyEstimator(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we give it an observation, it should return 4 probabilities for each action that can be taken in this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = env.reset()\n",
    "policy_estimator.predict(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this policy estimator is newly created, i.e. untrained, it should return a more or less uniformly distributed policy, where all actions probabilities are more or less equal. In this case all probabilities should be around 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select action\n",
    "\n",
    "Instead of the $\\epsilon$-greedy policy that we have used before, we are now going to select an action randomly according to the distribution given by the policy estimator. So, actions with a higher probability will be selected more often.\n",
    "\n",
    "This selection can be implemented with the [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) function. Its first parameter should be the number of actions to choose from (you can use `env.action_space.n`), and parameter `p` should be set to the probabilities of each action.\n",
    "\n",
    "Use the `policy_estimator` to predict action probabilities for the given `observation` using the `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation):\n",
    "    global env, policy_estimator\n",
    "    #### START CODE ####\n",
    "    ...\n",
    "    ...\n",
    "    #### END CODE ####\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works. If you run the following cell, you should get a single integer between 0 and 4 as a result. Re-run the cell to check if it is indeed randomly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = env.reset()\n",
    "action = random_policy(observation)\n",
    "assert 0 <= action < 4\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected returns\n",
    "\n",
    "Once the policy estimator is trained it should represent a policy that maximizes the expected return. So, for training we need to know the return $G_t$ at each timestep $t$, which is defined as:\n",
    "\n",
    "$$\\begin{align}\n",
    "G_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots \\\\\n",
    "    &= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align}$$\n",
    "\n",
    "We have the rewards $R_t$ gathered during the episode, so we can easily compute the $G_t$ for each step by traversing the rewards backwards. We start at $t = T$ (the terminal state) and go backward to $t = 0$. Remember, $G_T$ is 0, i.e. the terminal state has no expected return.\n",
    "\n",
    "This process is called discounting the rewards. You have to implement this in the following `discount` function. Note: in python you can simply reverse a list using the built-in function `reversed(a)` and you can insert an item at the start of a list with `a.insert(0, v)`.\n",
    "\n",
    "The extra dimension that is added is to make the array shape correct for usage in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rewards, gamma):\n",
    "    expected_returns = []\n",
    "\n",
    "    ### START CODE ###\n",
    "    # Initialize G_t to 0 for t = T\n",
    "    ...\n",
    "\n",
    "    # Reverse through rewards\n",
    "    ...\n",
    "        # Compute new G_t using current reward\n",
    "        ...\n",
    "        # Insert reward in front of expected_returns\n",
    "        ...\n",
    "\n",
    "    ### END CODE ###\n",
    "\n",
    "    # Convert to float32 NumPy array\n",
    "    expected_returns = np.array(expected_returns, dtype=np.float32)\n",
    "\n",
    "    # Add additional dimension\n",
    "    return np.expand_dims(expected_returns, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works by running this small unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([0, 1, 2, 3, 0, -3])\n",
    "discount(rewards, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be exactly:\n",
    "\n",
    "    array([[ 2.93553],\n",
    "           [ 3.2617 ],\n",
    "           [ 2.513  ],\n",
    "           [ 0.57   ],\n",
    "           [-2.7    ],\n",
    "           [-3.     ]], dtype=float32)\n",
    "\n",
    "If this is not the case (besides floating point rounding errors), then check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the remainder of this exercise we will use the same discount factor, called `gamma`, with a value of 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps\n",
    "\n",
    "Discounting rewards can only be done at the end of an episode, when the terminal state has been reached. We will define an `on_episode_end` callback function for our `run_environment` helper function to train the estimator.\n",
    "\n",
    "In this callback function you first have to discount the rewards to get the returns. Then we can hand all the required information to the `train` function of the `policy_estimator` to train it one step (see above for the function specification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_episode_end(episode, observations, actions, rewards, length, score):\n",
    "    #### START CODE ####\n",
    "    # Discount rewards\n",
    "    ...\n",
    "    # Train policy estimator\n",
    "    ...\n",
    "    #### END CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "That's all we need. What is left is to run a number of episodes to train the policy estimator. Simply execute the following cell and take a break. It could take a few minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_environment(env, 500, random_policy, on_episode_end=on_episode_end)\n",
    "\n",
    "policy_estimator.save_weights('lunar-reinforce.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "During evaluation we no longer want to explore, but only exploit. So, instead of choosing an action randomly we want to take the action with the highest probability. Like in the DQN algorithm we can simply use the [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) function for this.\n",
    "\n",
    "Finish the following policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(observation):\n",
    "    global policy_estimator\n",
    "    #### START CODE ####\n",
    "    # Predict action probabilities with policy estimator\n",
    "    ...\n",
    "    # Select action with highest probability\n",
    "    ...\n",
    "    #### END CODE ####\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how an untrained estimator performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_estimator = PolicyEstimator(env)\n",
    "eval_length, eval_score = evaluate(env, greedy_policy)\n",
    "print(f'Evaluation: length {eval_length}, score {eval_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's really bad. It hardly does anything to prevent crashing instantly.\n",
    "\n",
    "And, now look at how it performs after 500 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_estimator.load_weights('lunar-reinforce.pth')\n",
    "eval_length, eval_score = evaluate(env, greedy_policy)\n",
    "print(f'Evaluation: length {eval_length}, score {eval_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot better, it no longer crashes immediately, but still not optimal. It now hovers for a long time without landing.\n",
    "\n",
    "To get a lot better performance we have to train it for a lot longer. This is how it performs after being trained for 5000 episodes, which took about 1 hour and 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_estimator.load_weights('trained/lunar-reinforce.pth')\n",
    "eval_length, eval_score = evaluate(env, greedy_policy)\n",
    "print(f'Evaluation: length {eval_length}, score {eval_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is all that was needed to train using the REINFORCE algorithm. It is very simple, but it needs a lot of training to converge to an optimal solution.\n",
    "\n",
    "This algorithm, however, is the basis for a lot more advanced (and state-of-the-art) algorithms. Let's take a look at those."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
