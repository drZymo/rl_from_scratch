{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Q Network\n",
    "\n",
    "Welcome to the third exercise in this series. In the previous exercises we worked with relatively simple environments that were solvable quickly because the number of states was limited. In this exercise we will create an agent that learns a policy for an environment with lots of states. We will still be estimating the action values, but instead of storing them in a table we will be approximating them with a neural network.\n",
    "\n",
    "The algorithm is called \"Deep Q-learning with Experience Replay\" and the paper [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf) describes it well. The pseudo code of the algorithm is as follows.\n",
    "\n",
    "<img src=\"figures/dqn.png\" width=\"75%\" />\n",
    "\n",
    "We will implement it step-by-step.\n",
    "\n",
    "Let's first include a number of necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be displaying some nice screenshots here, but we need to initialize the library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_display\n",
    "init_display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar lander\n",
    "\n",
    "We will be using another Gym environment, this time we will use the `LunarLander` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env._max_episode_steps = 500\n",
    "\n",
    "print(f'Observation: {env.observation_space}, {env.observation_space.dtype}')\n",
    "print(f'Action:      {env.action_space}, {env.action_space.dtype}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the observation/state of this environment is a `Box`, which basically means a multi-dimensional array. This time with one dimension (i.e. it is a vector) of 8 floating point values (instead of a single integer) each with possible value between -inf and +inf. This means that the number of combinations infinite and not feasible to estimate with a table. We will have to use function approximation.\n",
    "\n",
    "The action space is (like before) an integer with 4 discrete values.\n",
    "\n",
    "**Note**: the change in maximum episode steps is done to improve training speed in this exercise. With the default of 1000 steps the training time would be significantly longer and we don't have time for that. It won't affect the end result too much.\n",
    "\n",
    "We can render the observation to an image to get a better feel for what the environment looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = env.reset()\n",
    "for _ in range(40):\n",
    "    env.step(0)\n",
    "for _ in range(4):\n",
    "    env.step(2)\n",
    "print(observation)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a moon lander and a landing zone marked with two yellow flags.\n",
    "\n",
    "The goal of our agent is to land the lander in the target zone. The lander has 3 thrusters, one vertical (downwards) and two horizontal to the left and right. The agent has 4 actions, one for firing each thruster and one action to do nothing. So there is at most one thruster active at all times.\n",
    "\n",
    "If it lands too fast, it crashes and you'll get a negative reward. If it lands safely you get a positive reward. For all the fuel you spend you will get a penalty as well. But, we don't have a model, so we pretend we don't know all this ;).\n",
    "\n",
    "The observation you get is a list of distances and velocities compared to the ground. It is not really important which value is what, because our agent will figure that out by itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "We will use an neural network as our function approximator. It takes too much time to also introduce the concepts of a neural network, so we will use an already existing implemention. It is implemented with [PyTorch](https://pytorch.org/). If you are finished early you can take a look at the implementation in the [`estimator.py`](estimator.py) file.\n",
    "\n",
    "This neural network accepts observations from the environment as input, and will return predicted values for that state. I.e. it approximates the function $v(s)$.\n",
    "\n",
    "Let's create a new instance and see it's initial prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimator import ActionValueEstimator\n",
    "\n",
    "estimator = ActionValueEstimator(env)\n",
    "\n",
    "# Reset the environment to get the initial observation\n",
    "observation, _ = env.reset()\n",
    "\n",
    "# Predict action value for all action in this state\n",
    "action_values = estimator.predict(observation)\n",
    "print(f'action values = {action_values}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of 4 action values with random values. If you run it again the values should be different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy policy\n",
    "\n",
    "Like in the previous exercise we will need an $\\epsilon$-greedy policy to explore during training. This time the policy will be based on the output of the action value estimator model (`estimator.predict`) given an observation. Implement the epsilon greedy policy using [`np.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html), `env.action_space.sample()`, and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, estimator, observation, epsilon):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Implement epsilon greedy algorithm.\n",
    "    # With probability `epsilon` take a random action,\n",
    "    # otherwise take the action with the highest probability.\n",
    "    ...\n",
    "        \n",
    "    ### END CODE ###\n",
    "    return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target action values\n",
    "\n",
    "Training of the model will be done on batches of experience (i.e. actor-environment interactions). These batches constain:\n",
    "\n",
    "- `observation`: The (current) observation, i.e. $S_t$\n",
    "- `action`: The action taken, i.e. $A_t$\n",
    "- `reward`: The reward returned by the environment after performing the action, i.e. $R_{t+1}$\n",
    "- `next_observation`: The next observation after the action is performed, i.e. $S_{t+1}$\n",
    "- `done`: Wheter or not the episode if finished. In other words, if the `next_observation` is of a terminal state.\n",
    "\n",
    "From such a batch we have to compute the target values for the neural network. This means we have to compute $Q(S_t,A_t)$. Like before, we will be using the the Q-learning algorithm, so our target will be bootstrapped with the prediction of the next observation.\n",
    "\n",
    "$$ R_t + \\gamma \\max_a Q(S_{t+1}, a) $$\n",
    "\n",
    "Implement that in the following function. You have to use `estimator.predict(observations)` to get the action values for the next observations. Then use [`np.amax`](https://numpy.org/doc/stable/reference/generated/numpy.amax.html) to get the value of the action with the highest value. Make sure that you set `axis=1` to get the maximum of each sample and `keepdims=True` to keep the shapes of the arrays in order.\n",
    "\n",
    "The value of terminal states is always 0 (remember $G_T = 0$), so $Q(S_T, a)$ must be 0 as well. You can use the `dones` array to correctly filter out those values. `dones` is an array of booleans, but similar to the C programming language you can use them as integers. A `True` value is processed as a `1` and a `False` value is processed as `0`. So, if you multiply the maximum action value (`max_next_q`) with `(1-dones)` you will end up with a valid maximum action value in samples that do not end in a terminal state, otherwise the result is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(estimator, rewards, next_observations, dones, gamma):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Estimate the action values for the next observation using the prediction model.\n",
    "    next_q = ...\n",
    "    \n",
    "    # Find the maximum action value.\n",
    "    max_next_q = ...\n",
    "    \n",
    "    # Compute the target action value as described above.\n",
    "    # Take care of the terminal state.\n",
    "    target_q = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    return target_q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test this. Run the following cell and see if your implementation is correct. A set of pre-defined input files is used to make sure the situation is always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ActionValueEstimator(env)\n",
    "estimator.load_weights('data/lunar_test_model.pth')\n",
    "\n",
    "r = np.load('data/lunar_test_rewards.npy')\n",
    "o = np.load('data/lunar_test_next_observations.npy')\n",
    "d = np.load('data/lunar_test_dones.npy')\n",
    "compute_target_values(estimator, r, o, d, 0.99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this test should be exactly equal to:\n",
    "\n",
    "    array([[84.27161251],\n",
    "           [84.83388991],\n",
    "           [85.45931969]])\n",
    "\n",
    "Note that it's shape is `(3,1)`, i.e. a single value for each of the three samples.\n",
    "\n",
    "If your output is not the same, then check your implementation! Make sure the shapes of the intermediate steps are correct. Check this using for instance `print(max_next_q.shape)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step\n",
    "\n",
    "We now have nearly all the pieces in place to start training. The last missing piece is the training step itself.\n",
    "\n",
    "The given neural network class has a dedicated function for fitting the neural network to a given dataset. It can be invoked as follows.\n",
    "\n",
    "    estimator.train(observations, actions, values)\n",
    "\n",
    "It uses [PyTorch](https://pytorch.org/) to automatically:\n",
    "- compute the output of the network for the given input data (i.e. predict),\n",
    "- compute the loss of the output using the given target values,\n",
    "- compute the gradients of the network weights with respect to the loss,\n",
    "- update the weights of the network using an optimizer\n",
    "\n",
    "Each time you call `train` the estimator will change slightly toward better fitting to the given data. Each time you do this is called an epoch. In other types of machine learning you usually run multiple epochs to fit your network perfectly to the given data. For our RL problem we only want to run one epoch and then gather new data with an updated model.\n",
    "\n",
    "Implement the `train` function below. It performs two steps. First we need to call the earlier implemented `compute_target_values()` function to get the target values for our estimator, and then call the `train` function of that estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(estimator, observations, actions, rewards, next_observations, dones, gamma):\n",
    "    ### START CODE ###\n",
    "\n",
    "    # Compute target values\n",
    "    target_q = ...\n",
    "\n",
    "    # Train the estimator to this data.\n",
    "    # It needs both the `observations` and `actions` as input parameters to the neural network\n",
    "    # and it needs `target_q` as target for the output of the neural network\n",
    "    ...\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, we can implement the training loop. Let's define a number of hyperparameters for our model, that we will explain later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.3\n",
    "gamma = 0.99\n",
    "batch_size = 256\n",
    "max_time = 300 # seconds\n",
    "max_episode_length = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate the estimator so we can start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ActionValueEstimator(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "As explained in the presentation, we will be using experience replay. In other words, the data gathered while interacting with the environment is stored in a big buffer with maximum capacity. From this buffer we take random batches to train on.\n",
    "\n",
    "We will use the class `ExperienceBuffer` for this, which is implemented in the [`utils.py`](utils.py) file. If you want you can take a look how it works. It is basically a big queue of data with a maximum capacity. When new data is added the oldest data is removed/overwritten.\n",
    "\n",
    "It has two important functions that you need to use:\n",
    "- `experience_buffer.append(observation, action, reward, next_observation, done)` to append the data of a single step.\n",
    "- `experience_buffer.sample(batch_size)` to get a random batch of size `batch_size`.\n",
    "\n",
    "Let's create an instance that can store 10000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ExperienceBuffer\n",
    "experience_buffer = ExperienceBuffer(10000, env.observation_space.shape, (1,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop\n",
    "\n",
    "The main training loop basically consists of three steps.\n",
    "1. Perform an action by using the $\\epsilon$-greedy policy. (Use `select_action` and `env.step`)\n",
    "2. Store experience in the buffer. (Use `experience_buffer.append`)\n",
    "3. Take a random batch from the experience buffer and train on it. (Use `experience_buffer.sample` and `train`)\n",
    "\n",
    "When an episode is finished we start a new episode and continue with these steps.\n",
    "\n",
    "There is only one problem in the LunarLander environment that is annoying during training. The environment does not stop automatically when the agent takes too long to finish the task. An episode can in some situations take >10000 steps, which means we have to wait quite a while before the episode is finished. To speed it all up we have to add an 'early stop' to the loop. If the episode takes more than `max_episode_length` steps it should abort.\n",
    "\n",
    "Fill in the required steps below and run it. It will show you the progess it makes per episode. You should see the scores go up on average.\n",
    "\n",
    "**Running this training loop takes 5 minutes.** Take a coffee/tea break and come back later to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "episode = 0\n",
    "while (datetime.now() - start_time).total_seconds() < max_time:\n",
    "    episode_length, episode_score = 0, 0\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        ### START CODE ###\n",
    "        \n",
    "        # Select an action using the epsilon-greedy policy\n",
    "        action = ...\n",
    "        \n",
    "        # Take action and get reward\n",
    "        next_observation, reward, done, _, _ = ...\n",
    "        \n",
    "        # End early\n",
    "        if episode_length >= max_episode_length - 1:\n",
    "            done = True\n",
    "\n",
    "        # Add to experience buffer\n",
    "        ...\n",
    "        \n",
    "        ### END CODE ###\n",
    "\n",
    "        if experience_buffer.is_filled(batch_size):\n",
    "            ### START CODE ###\n",
    "            \n",
    "            # Select a random batch from experience\n",
    "            observations, actions, rewards, next_observations, dones = ...\n",
    "\n",
    "            # Perform one train step on the batch\n",
    "            ...\n",
    "            \n",
    "            ### END CODE ###\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        observation = next_observation\n",
    "        episode_length += 1\n",
    "        episode_score += reward\n",
    "    \n",
    "    # Show statistics\n",
    "    t = (datetime.now() - start_time).total_seconds()\n",
    "    print(f'[{t:.1f}] #{episode}: length {episode_length}, score {episode_score:.0f}')\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "# Save the end result\n",
    "estimator.save_weights('lunar-dqn.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pfew, it finally finished training. By now the model should have been trained for 5 minutes or about 150+ episodes and the score should on average have been increasing. Time to evaluate.\n",
    "\n",
    "**Note:**\n",
    "If it didn't increase at all, then you are one of the unlucky few that had a model converged on a local (sub-)optimum. This sometimes happen in deep reinforcment learning, and in particular in unstable environments like this. There are ways to prevent this, like choosing less agressive learning rates and bigger batch sizes, but those will increase training time a lot. For now, the only thing you can do is to restart the entire notebook. If you have the time (maybe while listening to the next part of the lectures) you can click on `Kernel`->`Restart & Run All` and try again. But first read on to the end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "It took quite some time to train and that is only for a relatively small number of episodes. Let's see how well it performs anyway. We will use the following helper function to evaluate a model by taking greedy actions only (no more exploring, only exploiting) and reporting the final score. In the meantime we'll render the environment to show you what happens.\n",
    "\n",
    "Implement the missing parts. You can take a look at the \"greedy\" part of the `select_action` function for some additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_frame, update_frame\n",
    "\n",
    "def evaluate(estimator, env):\n",
    "    ### START CODE ###\n",
    "\n",
    "    # Reset environment\n",
    "    observation, _ = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "\n",
    "    done, score, length = False, 0, 0\n",
    "    frame = create_frame(env)\n",
    "\n",
    "    while not done:\n",
    "        ### START CODE ###\n",
    "        \n",
    "        # Greedy select an action based on the estimated action values\n",
    "        action = ...\n",
    "        \n",
    "        # Perform action\n",
    "        observation, reward, done, _, _ = ...\n",
    "        \n",
    "        ### END CODE ###\n",
    "        \n",
    "        # End early\n",
    "        if length >= max_episode_length - 1:\n",
    "            done = True\n",
    "\n",
    "        score += reward\n",
    "        length += 1\n",
    "        update_frame(frame)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how the randomly initialized model performs. It should be really bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ActionValueEstimator(env)\n",
    "score = evaluate(estimator, env)\n",
    "print(f'initial model: {score:.1f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well your trained model performs. We'll load the model as saved at the end of your training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.load_weights('lunar-dqn.pth')\n",
    "score = evaluate(estimator, env)\n",
    "print(f'partially trained model: {score:.1f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should perform a lot better. It should no longer instantly crash and burn. But it performs nowhere near optimal. For that we need to train it a lot longer. Here is the performance of a model that was trained for 4500 episodes with a decreasing value of epsilon (epsilon decay) to assist in training the later stages. This took more than an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.load_weights('trained/lunar-dqn.pth')\n",
    "score = evaluate(estimator, env)\n",
    "print(f'fully trained model: {score:.1f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have trained a model-free agent. Purely by interacting with the environment it learned what actions are best for each observation it receives. It takes quite some time to train, and it didn't event got that far. It requires a lot more time and experience to train to a decent score. But, the size of the observation is also a lot bigger than before. There are a number of things you can do to improve training time and sample efficieny (how well does it learn for a single interaction with the environment), but that's out-of-scope for this training.\n",
    "\n",
    "In the next and final exercise we will implement another way to train an agent using a neural network that allows more complex environments.\n",
    "\n",
    "If you are interested you can also take a look at the implemention of the neural network in the [`estimator.py`](estimator.py) file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
