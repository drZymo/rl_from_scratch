{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #5 - Stable Baselines3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final exercise we will use a library with implementations of various RL algorithms instead of implementing them ourselves. We will be using the [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/) library. It has implementations of a lot of algorithms using PyTorch, and the implementations have been tested against the original papers to make sure the results are the same.\n",
    "\n",
    "In this exercise we will use the [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm that is basically an upgraded Actor-Critic using multiple workers (A3C) and a smarter neural network gradient (TRPO). We will see that it converges a lot quicker on the `LunarLander-v2` environment.\n",
    "\n",
    "Let's start with including the necessary classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "One of the ways to improve the efficiency of Reinforcement Learning is by using multiple parallel environments. With Stable Baselines3 that is really simple to achieve. Take a look at the [Examples page](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html) for more details.\n",
    "\n",
    "Let's create a new environment that runs 4 instances simultaneously using the `make_vec_env` function. These vectorized environments coordinate the execution of multiple environments that run in separate processes. Make sure the `seed` parameter is set to a value (e.g. `0`), otherwise the initialization could break. The `make_vec_env` utility function will create a new vectorized environment using the `gymnasium` library we have used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "env = ...\n",
    "### END CODE ###\n",
    "\n",
    "# Show observation shape\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the output is now a batch of 4 observations, one for each environment.\n",
    "\n",
    "With this environment we can create a model. This model will check the observation and action spaces of the environment and configures the neural networks accordingly. We want to create a new `PPO` instance with the `'MlpPolicy'` policy network configuration. This is a simple 'multi-layer perceptron' network, perfectly suitable for this task. Make sure `verbose` is set to `1` to show some output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "model = ...\n",
    "### END CODE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start training, which is done with the `learn` function of the model. Train for `200000` time steps and save the results. It should take about 5 minutes on this server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "# Learn the model for 200_000 steps\n",
    "...\n",
    "### END CODE ###\n",
    "\n",
    "model.save('lunar-ppo')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's take a look at the result.\n",
    "\n",
    "First we need to import the same utility functions to show the images as we did in previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_display, create_frame, update_frame\n",
    "init_display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how well it performs with a single environment, not with a vectorized environment. So, we have to create a new environment. Let's use `gymnasium` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "env = gymnasium.make('LunarLander-v2', render_mode='rgb_array')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict`. Hidden state (for recurrent neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_file):\n",
    "    # Load model from disk\n",
    "    model = PPO.load(model_file)\n",
    "\n",
    "    ### START CODE ###\n",
    "\n",
    "    # Reset environment\n",
    "    obs, _ = ...\n",
    "\n",
    "    ### END CODE ###\n",
    "\n",
    "    # Show initial frame\n",
    "    frame = create_frame(env)\n",
    "\n",
    "    # Start loop\n",
    "    done, score, length = False, 0, 0\n",
    "    while not done:\n",
    "        # Update frame with current state\n",
    "        update_frame(frame)\n",
    "\n",
    "        ### START CODE ###\n",
    "        \n",
    "        # Get action from model based on the current state\n",
    "        action, _ = ...\n",
    "\n",
    "        # Perform action in environment and collect next state and reward\n",
    "        obs, reward, done, _, _ = ...\n",
    "\n",
    "        ### END CODE ###\n",
    "\n",
    "        # End early\n",
    "        if length >= 500:\n",
    "            reward += -100\n",
    "            done = True\n",
    "\n",
    "        # Next iteration\n",
    "        score += reward\n",
    "        length += 1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('lunar-ppo')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should have performed quite well already, and that after only 5 minutes.\n",
    "\n",
    "If you let it train for 1.5 million time steps with 8 environments in parallel you can achieve the following result. This took less than 20 minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate('trained/lunar-ppo')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Well, that was easy, right?\n",
    "\n",
    "We have seen how the basic algorithms work in the previous exercises. The state-of-the-art algorithms are a bit more work to implement, but luckily these libraries allow us to use them as well with minimum effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
