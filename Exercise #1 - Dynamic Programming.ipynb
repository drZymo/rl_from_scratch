{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dynamic programming\n",
    "\n",
    "Welcome to the first exercise, where we will apply Dynamic Programming to find an optimal policy for an agent in an environment of which the dynamics are fully known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing the necessary libraries, classes and functions.\n",
    "- `numpy` is a numeric library we will use for working with vectors and matrices (see [here](https://numpy.org/doc/stable/) for the reference manual).\n",
    "- The `mdp` package contains the implementation of the environment we will be working with and some helper functions for displaying the results.\n",
    "\n",
    "Run the next cell, by pressing `SHIFT+ENTER` or clicking the run button in the toolbar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdp import GridWorldMDP, argmax, plot_world, plot_values, plot_values_and_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid world\n",
    "\n",
    "The environment for this exercise is a 2-dimensional grid world. With a central area surrounded by a cliff. Your agent walks around in this world and its goal is to reach the safe zone.\n",
    "\n",
    "Run the next cell to see the layout of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent starts in the bottom left corner (red cell) and it needs to reach the safe zone at the bottom right corner (green cell). At every time step the agent can move `up`, `down`, `left`, or `right`. If the agent falls of the cliff (gray cells) then it dies and gets a reward of -100. When it reaches the safe zone it gets a reward of +1. In all other cases it gets a reward of -1 to incentivize the agent to reach the safe zone as fast as possible. When the agent is dead or when it has reached the safe zone it is in a terminal state. There all movements will end up in the same cell with zero reward.\n",
    "\n",
    "#### Wind\n",
    "\n",
    "To make it more interesting there is a strong wind blowing downwards in this world. As a result there is a 20% chance that the agent is blown down one cell during his move. So a move upwards has 20% chance of ending in the same state, and a move right  in state (x:2, y:1) has a 20% chance of ending up in cell (x:3, y:2). This means that moving along the cliff at the bottom is really risky because the agent can be pushed off the cliff by the wind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interface\n",
    "\n",
    "The already imported class `GridWorldMDP` is the interface to the environment. It has a number of fields that are of interest.\n",
    "- `STATES`: A list of all the states of the environment. These are simply the cells of the world numbered from top-left to bottom-right. The state represents the cell where the agent is currently positioned. So state `8` means the agent is positioned in the top-left corner of the blue area in the above image.\n",
    "- `ACTIONS`: A list of all the actions that are possible. Basically this is `['up', 'right', 'down', 'left']`.\n",
    "- `REWARDS`: A list of all the possible rewards that can be received from the environment (i.e. `[0, 1, -1, -100]`)\n",
    "\n",
    "Next to that it also provides the following functions:\n",
    "- `p(state, action, next_state, reward)`: returns the dynamics of the environment. This function will return the probability of a transition from `state` to `next_state` when `action` is performed and `reward` is returned.\n",
    "- `A(state)`: Returns the list of actions allows in state `state`.\n",
    "\n",
    "In other words the `GridWorldMDP` class describes the complete MDP of this environment (similar to the `RabbitMDP` class used in the demo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "\n",
    "<img src=\"figures/policy_iteration.png\" width=\"50%\" align=\"right\"> We are going to implement the Policy Iteration algorithm as described on the right. This algorithm is split into three parts. \n",
    "\n",
    "1. Initialization.\n",
    "2. Policy Evaluation. Where we estimate the *state value function* $v_\\pi(s)$ for the current policy.\n",
    "3. Policy Improvement. Where we update the policy $\\pi(s,a)$, so it acts greedy with respect to the *action value function*  $q_\\pi(s,a)$.\n",
    "\n",
    "Recall that the value functions can be written recursively as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_\\pi(s,a) & = \\sum_{s^\\prime \\in S} \\sum_{r \\in R} {p(s, a, s^\\prime, r) (r + \\gamma v_\\pi(s^\\prime))} \\\\\n",
    "v_\\pi(s) & = \\sum_{a \\in A} {\\pi(s, a) q_\\pi(s, a)}\n",
    "\\end{align}$$\n",
    "\n",
    "This means that we can compute the *action value function* $q_\\pi(s,a)$ with a known *state value function* $v_\\pi(s)$ and vice versa.\n",
    "\n",
    "In this exercise we will implement the *state value function* $v_\\pi(s)$ using a lookup table with estimated values for all the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "In the first part of the algorithm we are going to update the lookup table `v` that contains the estimated values of all the states. For this we are first going to make an implementation of $q_\\pi(s,a)$, which we will use to update the table.\n",
    "\n",
    "$$q_\\pi(s,a) = \\sum_{s^\\prime \\in S} \\sum_{r \\in R} {p(s, a, s^\\prime, r) (r + \\gamma v_\\pi(s^\\prime))}$$\n",
    "\n",
    "The current state value estimates are stored in parameter `v` which is an array that can simply be indexed with a state number to return the estimated value of that state. The `env` parameter is a `GridWorldMDP` intance. You can use `env.STATES` or `env.REWARDS` to get a list of all the states or rewards in the MDP. You can use `env.p()` to get the transition probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_value(env, state, action, v, gamma):\n",
    "    q = 0\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Iterate over all possible next states and rewards and compute the expected return of that transition.\n",
    "    for ...:\n",
    "        for ...:\n",
    "            # compute the value of this transition\n",
    "            q += ...\n",
    "    ### END CODE ###\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if you implemented this correctly with a quick unit test. If you run the next cell it should result in `-20.8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldMDP()\n",
    "v = np.zeros(len(env.STATES))\n",
    "\n",
    "compute_action_value(env, 16, env._DOWN, v, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `compute_action_value` we can now compute the state values for the current policy.\n",
    "\n",
    "$$v_\\pi(s)  = \\sum_{a \\in A} {\\pi(s, a) q_\\pi(s, a)}$$\n",
    "\n",
    "Again the current state value estimates are stored in parameter `v` and the `env` parameter is an instance of `GridWorldMDP`. You can use `env.ACTIONS` to get a list of all possible actions. The current policy $\\pi$ is stored in a 2-dimensional array `pi`. It can be indexed with a state and action pair (e.g. `pi[s, a]`) to get the probability for selecting action `a` in state `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value(env, state, v, pi, gamma):\n",
    "    ### START CODE ###\n",
    "    value = 0\n",
    "    # Iterate over all actions\n",
    "    for ...:\n",
    "        # Use pi and compute_action_value to compute expected return of this action\n",
    "        value += ...\n",
    "    ### START CODE ###\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small unit test to see if the implementation is correct. If you run this the result should be `-5.95`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldMDP()\n",
    "v = np.zeros(len(env.STATES))\n",
    "pi = np.ones((len(env.STATES), len(env.ACTIONS))) / len(env.ACTIONS)\n",
    "\n",
    "compute_state_value(env, 16, v, pi, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to combine this in the Policy Evaluation loop, which will keep updating the state values until the table converges to a certain accuracy $\\theta$ (i.e. `theta`). This is already implemented; you don't have to do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, v, pi, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.STATES:\n",
    "            vs_old = v[s]\n",
    "            v[s] = compute_state_value(env, s, v, pi, gamma)\n",
    "            delta = max(delta, abs(vs_old - v[s]))\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it for one run. We start with initial state value estimates of 0, and a policy with equal distribution for all actions, which are displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldMDP()\n",
    "v = np.zeros(len(env.STATES))\n",
    "pi = np.ones((len(env.STATES), len(env.ACTIONS))) / len(env.ACTIONS)\n",
    "plot_values_and_policy(v, env, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate this policy to get an updated state value table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(env, v, pi, 0.9, 1e-7)\n",
    "\n",
    "plot_values(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "Now that the value functions are known for the current policy, we can update the policy so it selects actions that maximum the total reward. This simply means the policy should select actions with the highest action value in the given state (i.e. act greedy with respect to $q_\\pi(s,a)$. If more than one action has the highest action value then it should select them with equal probability. For example, if two actions have the highest action value, then the probability for selection each action should be 0.5.\n",
    "\n",
    "The policy is stored in a two dimensional array `pi`. One row for each state, and a column for each action. We are going to update the policy state by state, so we are going to replace the table row by row.\n",
    "\n",
    "The function `argmax` is provided to get a list of all actions that have the maximum value a. Don't use the numpy `argmax` function, because that only gives the index of the first maximum value, not all of them.\n",
    "\n",
    "Remember that the sum of all probabilities in a row should be equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedify_policy(env, v, pi, state, gamma):\n",
    "    new_pi = np.zeros_like(pi[state])\n",
    "    \n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Compute the action values for all actions in this state.\n",
    "    q_s = [... for action in env.ACTIONS]\n",
    "    \n",
    "    # Get the list of actions that have the maximum value\n",
    "    max_actions = ...\n",
    "    \n",
    "    # Set the probability for all actions in max_action\n",
    "    # All these actions should have the same probability and the sum should be 1.\n",
    "    new_pi[max_actions] = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    \n",
    "    pi[state] = new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test to your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldMDP()\n",
    "v = np.zeros(len(env.STATES))\n",
    "pi = np.ones((len(env.STATES), len(env.ACTIONS))) / len(env.ACTIONS)\n",
    "\n",
    "print(pi[23])\n",
    "greedify_policy(env, v, pi, 23, 0.9)\n",
    "print(pi[23])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output you should see the policy for state 23 change from `[0.25 0.25 0.25 0.25]` (i.e. completely random) to `[0.5 0.  0.  0.5]` (only up and left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the full Policy Improvement step, which will greedify the policy for all states and return whether or not it is stable (i.e. didn't change). This is already implemented; you don't have to do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(env, v, pi, gamma):\n",
    "    is_stable = True\n",
    "    for state in env.STATES:\n",
    "        old = pi[state].copy()\n",
    "        greedify_policy(env, v, pi, state, gamma)\n",
    "        if not np.array_equal(pi[state], old):\n",
    "            is_stable = False\n",
    "    return is_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see one iteration through the algorithm in action. We first start with a state value estimates of 0 for all states and a completely random policy. The following cell will display that nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, theta = 0.9, 1e-7\n",
    "\n",
    "env = GridWorldMDP()\n",
    "v = np.zeros(len(env.STATES))\n",
    "pi = np.ones((len(env.STATES), len(env.ACTIONS))) / len(env.ACTIONS)\n",
    "\n",
    "plot_values_and_policy(v, env, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then evaluate and improve the policy once and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(env, v, pi, gamma, theta)\n",
    "is_stable = improve_policy(env, v, pi, gamma)\n",
    "\n",
    "plot_values_and_policy(v, env, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see that the values for all the 'blue' cells are computed and very negative (< -60). All terminal states (including the end state) should have value 0.\n",
    "\n",
    "The policy that maximizes the return with these value functions will already walk a route towards the end state. It is not optimal however. For that we have to iterate a few more times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this is it. We can now implement the entire algorithm. Fill in the code below and run it to obtain the optimal policy for this grid world. Be patient it can take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, theta = 0.9, 1e-7\n",
    "\n",
    "env = GridWorldMDP()\n",
    "v = np.zeros(len(env.STATES))\n",
    "pi = np.ones((len(env.STATES), len(env.ACTIONS))) / len(env.ACTIONS)\n",
    "\n",
    "is_stable = False\n",
    "while not is_stable:\n",
    "    ### START CODE ###\n",
    "    ...\n",
    "    is_stable = ...\n",
    "    ### END CODE ###    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now display the resulting value function and policy you can see that the policy has changed a little, in particular in the cells on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values_and_policy(v, env, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in cell (x:5, y:3) the direction of the arrow has flipped. This is counter-intuitive. You would think moving down would be the best action, because you are directly above the end goal. However, there is a chance the wind blows you down yet another cell into the cliff. It is safer to move up and then down again with a chance you move down two cells right on the end cell.\n",
    "\n",
    "This can be seen in the action values for that that state. (Note: actions are order `up`, `right`, `down`, `left`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 3*7+5\n",
    "qs = [compute_action_value(env, state, action, v, gamma) for action in env.ACTIONS]\n",
    "print(f'q({state}) = {qs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be equal to:\n",
    "\n",
    "    q(26) = [-4.748010610080134, -100.0, -19.2, -23.6933687002657]\n",
    "\n",
    "This means that going up will result in an expected return of -4.7 and going down in -19.2, which is clearly worse than going up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's it for this exercise. You now understand how to estimate the value functions for a given policy, and how to define a policy given those value functions.\n",
    "\n",
    "This all depends on a complete understanding of the dynamics of the environment, i.e. a model, in the next exercise we will implement an agent that does not rely on a model and simply learns an optimal policy by interacting with the environment. A model-free algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
