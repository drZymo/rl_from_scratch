{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Functions\n",
    "\n",
    "In this notebook we will show you how the state value function $V(s)$ and action value function $Q(s,a)$ of an MDP can be computed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/RabbitMDP.png\" width=\"50%\" align=\"right\" />\n",
    "\n",
    "## Explore MDP\n",
    "\n",
    "Let's start with exploring the interface of the Rabbit MDP that was discussed in the lecture. It is depicted on the right.\n",
    "\n",
    "We first need to import the class that implements it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import RabbitMDP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a new instance with which we can interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = RabbitMDP()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what states and actions are available in this MDP by looking at its public field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idle', 'hungry', 'eating', 'dead']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wakeup', 'go eat', 'stay', 'go home']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.ACTIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 4 states, and also 4 actions.\n",
    "\n",
    "Each action results in a reward. The different reward values that are possible are listed in the field `REWARDS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, -1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.REWARDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, not all actions allowed in all states. The function `A(s)` returns the list of actions allowed in state `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idle\n",
      "  wakeup\n",
      "hungry\n",
      "  go eat\n",
      "  stay\n",
      "eating\n",
      "  go eat\n",
      "  go home\n",
      "dead\n"
     ]
    }
   ],
   "source": [
    "for s in mdp.STATES:\n",
    "    print(s)\n",
    "    for a in mdp.A(s):\n",
    "        print(f'  {a}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p function\n",
    "\n",
    "The other very important component of an MDP is its state-transition function $p(s', r | s, a)$. It returns the probability of ending in state $s'$ with reward $r$ when starting from state $s$ and taking action $a$.\n",
    "\n",
    "In the MDP class this is implemented with function `p(state, action, next_state, reward)`. Let's see if it returns what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.p('hungry', 'go eat', 'eating', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.p('hungry', 'go eat', 'dead', -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, these transitions match what we see in the diagram above.\n",
    "\n",
    "Transitions that are not allowed have a probability of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.p('hungry', 'go eat', 'eating', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.p('hungry', 'go eat', 'idle', 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities of all transitions from state $s$ by taking a action $a$ should sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.8\n",
      "0\n",
      "0\n",
      "0\n",
      "0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for next_s in mdp.STATES:\n",
    "    for r in mdp.REWARDS:\n",
    "        p = mdp.p('hungry', 'go eat', next_s, r)\n",
    "        total += p\n",
    "        print(p)\n",
    "total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting\n",
    "\n",
    "Interacting with the environment (MDP) goes as follows.\n",
    "\n",
    "The environment starts in a certain state. In this case `idle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idle'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mdp.STATES[0]\n",
    "s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there are a number of actions possible in that state.\n",
    "In this case only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wakeup']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.A(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can now determine which action to take using its policy $\\pi(a|s)$.\n",
    "\n",
    "This policy will return the probability of taking each action. Since we have only one action in this state, the probability is `1` for action `wakeup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wakeup'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = mdp.A(s)[0]\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the environment will tell us which state we will end up in using its `p` function. For each possible next state and reward combination it gives a probability of that transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungry, 0: 1\n"
     ]
    }
   ],
   "source": [
    "for s_next in mdp.STATES:\n",
    "    for r in mdp.REWARDS:\n",
    "        p = mdp.p(s, a, s_next, r)\n",
    "        if p > 0:\n",
    "            print(f'{s_next}, {r}: {p}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one transition has a probability which goes to next state `hungy` with a reward of `0`.\n",
    "\n",
    "So now the current state is updated and we have another set of allowed actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go eat', 'stay']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'hungry'\n",
    "mdp.A(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the agent can choose between two actions. Let's assume its policy is completely random, so it has a 50% chance of taking each action. In this case is chooses the `stay` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'stay'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see again what transitions are possible by the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungry, 0: 0.9\n",
      "dead, -1: 0.1\n"
     ]
    }
   ],
   "source": [
    "def show_transitions(s, a):\n",
    "    for s_next in mdp.STATES:\n",
    "        for r in mdp.REWARDS:\n",
    "            p = mdp.p(s, a, s_next, r)\n",
    "            if p > 0:\n",
    "                print(f'{s_next}, {r}: {p}')\n",
    "show_transitions(s, a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the environment will remain in the same state without a reward 90% probability. But, in 10% of the cases the environment can end up in the `dead` state with a negative reward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory\n",
    "\n",
    "A trajectory is a sequence of such interactions with the environment.\n",
    "\n",
    "$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \\dots, R_T, S_T$$\n",
    "\n",
    "For example we can take the following trajectory:\n",
    "\n",
    "    idle, wakeup, 0, hungry, stay, 0, hungry, go eat, +1, go home, 0, idle\n",
    "\n",
    "Over this trajectory the return `G` is equals to\n",
    "\n",
    "$$G = R_1 + R_2 + R_3 + R4 = 0 + 0 + 1 + 0 = 1$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition values\n",
    "\n",
    "We are going to compute the value of each state with the given functions.\n",
    "\n",
    "But, we are not there yet, so let's assume we know the values of each state upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = {'idle': 0.41213695806784034, 'hungry': 0.4579299534087115, 'eating': 0.06241200632828714, 'dead': 0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's start from the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idle'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mdp.STATES[0]\n",
    "s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of this state $V(idle)$ is the expected return when following policy $pi$. This can be computed by computing the value of each possible transition from this state multiplied with the probability that this transition occurs.\n",
    "\n",
    "In this state it is easy. As seen above there is only on possible transition. If taking action `wakeup` there is a 100% probability the environment ends up in state `hungry` with `0` reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'wakeup'\n",
    "s_next = 'hungry'\n",
    "r = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of this transition is the expected return of this transition. In other words, the reward you get immediately plus the expected return from the next state. However, this environment is continuous, so we have to apply discounting.\n",
    "\n",
    "$$ G = R_1 + \\gamma R_2 + \\gamma^2 R_3 \\dots$$\n",
    "\n",
    "For this example we use a value of 0.9, which more or less limits our horizon to 10 steps in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we know the value of the next state (`V['hungry']`), we can compute the value of this transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41213695806784034"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r + gamma * V[s_next]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is the same value as was already in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41213695806784034"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V['idle']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action values\n",
    "\n",
    "Now continue with the next state `hungry`. From this two actions are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go eat', 'stay']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'hungry'\n",
    "mdp.A(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each action we can take a look at the possible transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go eat\n",
      "eating, 1: 0.8\n",
      "dead, -1: 0.2\n",
      "stay\n",
      "hungry, 0: 0.9\n",
      "dead, -1: 0.1\n"
     ]
    }
   ],
   "source": [
    "a = 'go eat'\n",
    "print(a)\n",
    "show_transitions(s, a)\n",
    "\n",
    "a = 'stay'\n",
    "print(a)\n",
    "show_transitions(s, a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the value for the first action `go eat`. Two transitions are possible, let's compute the values for these two transitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0561708056954584\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "v1 = 1 + gamma * V['eating']\n",
    "print(v1)\n",
    "v2 = -1 + gamma * V['dead']\n",
    "print(v2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition to state `eating` has 80% probability and to state `dead` 20%. So the expected return of taking action `go eat` from state `hungry` is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6449366445563667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_go_eat = 0.8 * v1 + 0.2 * v2\n",
    "v_go_eat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other action, `stay`, also has two possible transitions, so the expected return for taking action `stay` in state `hungry` is equal to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27092326226105634"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = 0 + gamma * V['hungry']\n",
    "v2 = -1 + gamma * V['dead']\n",
    "v_stay = 0.9 * v1 + 0.1 * v2\n",
    "v_stay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are actuall the state-action values $q({go eat} | hungry)$ and $q(stay | hungry)$. In other words, the expected returns when taking action $a$ from state $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_go_eat = v_go_eat\n",
    "q_stay = v_stay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State values\n",
    "\n",
    "The policy of the agent determines the probability of taking each action. Because the policy determines what happens next from the current state, we can compute the expected return for state `hungry` when we follow the current policy.\n",
    "\n",
    "Our policy is completely random, so the probability of taking the two actions is 50% for each. So in 50% of the cases the return will be the expected return of taking action `go eat`, and in the other 50% of the cases the return will be the expected return of taking action `stay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4579299534087115"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_hungry = 0.5 * q_go_eat + 0.5 * q_stay\n",
    "v_hungry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the value of state `hungry` and is indeed equal to the value in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4579299534087115"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V['hungry']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
