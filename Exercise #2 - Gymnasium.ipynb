{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2 - Gymnasium\n",
    "\n",
    "In this exercise we will look at the [Gymnasium](https://gymnasium.farama.org/) library and create a few helper functions that we can use in the next exercises.\n",
    "\n",
    "Let's first import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already seen a demonstration how the environment can be used. If you need a refresher, then you can open `Demo #3a - Taxi.ipynb`.\n",
    "\n",
    "Let's create a new environment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run one episode\n",
    "\n",
    "First, we start with running a single episode. This means resetting the environment to its initial state and then using a policy to take steps in the environment until the episode is finished. We want to store all states, actions and rewards of the entire episode, so we can use that in later exercises to train our agents. You have to fill in the code below.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "First reset the given environment using its `reset` function. It returns a tuple of two items of which you can discard the second. Discarding can be done using the `_` variable. For instance `a, _ = (10, 20)` will assign `10` to `a` and discard the `20`. The first item it returns is the initial observation, i.e. how the environment displays its current state. Assign that to `observation` variable so we can use it in the loop.\n",
    "\n",
    "#### Loop\n",
    "\n",
    "We then enter the main loop, where we first have to get an action for the current observation using the given policy function. The `policy` parameter should reference a function, making it callable. This policy functiun should accept an observation and return an action (integer). Call the `policy` function with the current `observation` as input. We also want to store all observations received and actions taken, so use the `append` function of lists to add the `observation` to the `observations` list and the `action` to the `actions` list.\n",
    "\n",
    "Use the action to take a step in the environment using the `step` function of `env`. You only have to provide `action` as input. It will return a tuple of **5** items, of which the last can be discarded. The first 4 items we want to store in `next_observation`, `reward`, `done`, and `trunc`. Then add `reward` to the `rewards` list. The `done` and `trunc` variables are used later in the code and do not have to be added to a list.\n",
    "\n",
    "We also want to track the current episode length and the sum of the rewards received (also known as the return $G_T$). These values can be simply computed by calling the global function `len` with the `actions` list as input, and the `sum` function with the `rewards` list as input. Store the length in the `length` variable and the return in the `score` variable. We can't use `return`, because is a reserved keyword.\n",
    "\n",
    "Last step in the loop is the to assign `next_observation` to `observation`, so the next iteration uses the right input for the policy.\n",
    "\n",
    "Every environment step is also notified to the client using a callback function `on_step`, but that is already implemented for you. This will come in handy in later exercises.\n",
    "\n",
    "### Wrap-up\n",
    "\n",
    "After the loop has finished, there are two things that need to be done. First, compute the final length and score the same way as we did before. Then we have to convert the lists of gathered data to NumPy arrays. This makes them easier and faster to process later on. This can be simply done by calling the [`np.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) function with a list as input. You also have to specify the datatype of the items in the arrays using the `dtype` parameter. For the observations and rewards you should use the `np.float32` datatype, but for the actions you should use the `np.int32` datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env : gymnasium.Env, policy, on_step = None):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    #### START CODE ####\n",
    "    # Reset environment and assign observation\n",
    "    ...\n",
    "    #### END CODE ####\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        #### START CODE ####\n",
    "        # Get action from policy for current observation\n",
    "        ...\n",
    "        # Store observation and action\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        # Take step in environment and assign next_observation, reward, done, and trunc (discard last item)\n",
    "        ...\n",
    "        # Store reward\n",
    "        ...\n",
    "\n",
    "        # Compute current length and return\n",
    "        ...\n",
    "        ...\n",
    "        #### END CODE ####\n",
    "\n",
    "        # Stop the episode if it was truncated (i.e. max steps reached)\n",
    "        if trunc: done = True\n",
    "\n",
    "        # Call a callback function to notify the result of this single step\n",
    "        if on_step: on_step(observation, action, next_observation, reward, done, length, score)\n",
    "\n",
    "        #### START CODE ####\n",
    "        # Make next observation the current observation\n",
    "        ...\n",
    "        #### END CODE ####\n",
    "\n",
    "    #### START CODE ####\n",
    "    # Compute total length and return\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    #### END CODE #### \n",
    "\n",
    "    return length, score, observations, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can call this new function, we need to define a policy function. We will make a completely random policy. The Gymnasium environment object has two public properties `observation_space` and `action_space`. The latter has a nice function called `sample`, that will return a random sample from the action space. You don't have to know the type of action or the bounds, this function will do just what you need. So, call this function and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation):\n",
    "    #### START CODE ####\n",
    "    action = ...\n",
    "    #### END CODE ####\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if it all works as intended.\n",
    "\n",
    "Run the next cell. It should print a long list (200 lines) with rewards and scores of a random policy that plays the Taxi game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_step(observation, action, next_observation, reward, done, length, score):\n",
    "    print(f'  {reward=}, {score=}')\n",
    "\n",
    "length, score, observations, actions, rewards = run_episode(env, random_policy, on_step)\n",
    "observations.shape, actions.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell should be the shape of the observarions, actions, and rewards arrays and should be equal to.\n",
    "\n",
    "    ((201,), (200,), (200,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple episodes\n",
    "\n",
    "This was most of the work. We can now simply call the above function in a for loop to run multiple episodes.\n",
    "\n",
    "The function below will run `n_episodes` and shows a progress bar. It will also compute a moving average of the episode length and score (i.e. return). After every episode the client will be notified of the result using the `on_episode_end` callback function. This is all already implemented, you only have to fill in the call to the `run_episode` function. Don't forget to pass along the `policy` and the `on_step` parameters and store the results in the correct variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_environment(env : gymnasium.Env, n_episodes, policy, on_step = None, on_episode_end = None):\n",
    "    # Initialize moving averages\n",
    "    avg_factor = 0.99\n",
    "    avg_length = None\n",
    "    avg_score = None\n",
    "\n",
    "    # Show a progress bar using TQDM\n",
    "    E = trange(n_episodes)\n",
    "    for episode in E:\n",
    "        #### START CODE ####\n",
    "        # Run a single episode and assign output to correct variables\n",
    "        ...\n",
    "        #### END CODE ####\n",
    "\n",
    "        # Compute an show a moving average of the length and the score\n",
    "        avg_length = (avg_factor * avg_length + (1-avg_factor) * length) if avg_length else length\n",
    "        avg_score = (avg_factor * avg_score + (1-avg_factor) * score) if avg_score else score\n",
    "        E.set_postfix(avg_length=f'{avg_length:.1f}', avg_score=f'{avg_score:.1f}')\n",
    "        \n",
    "        # Call a callback function to notify the results of a single episode\n",
    "        if on_episode_end: on_episode_end(episode, observations, actions, rewards, length, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this we are going to use the same functions we used before. But now with an additional `on_episode_end` function to show the result of each episode. Run the next cell to see if everything is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_episode_end(episode, observations, actions, rewards, length, score):\n",
    "    print(f'ep #{episode}: {length=}, {score=}')\n",
    "\n",
    "run_environment(env, 2, random_policy, on_step, on_episode_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a progress bar that is updated quickly and shows two episodes being run including an average length and score. The averages are probably length 200 and average a very negative number. You should also see the output of the `on_step` and `on_episode_end` functions. For example:\n",
    "\n",
    "      reward=-1, score=-774\n",
    "      reward=-1, score=-775\n",
    "      reward=-1, score=-776\n",
    "    ep #1: length=200, score=-776\n",
    "\n",
    "If this is not the case, check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Next to running episodes and keeping track of all the data, we can also simply run a single episode and visualize the result. As demonstrated, the Gymnasium environments have the option to render the observations in a human friendly way.\n",
    "\n",
    "We can simply reuse the function we already created in the following way, using two utility functions that initialize and update an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_frame, update_frame\n",
    "\n",
    "def evaluate(env: gymnasium.Env, policy):\n",
    "    # Reset the environment and create an image of the current observation\n",
    "    env.reset()\n",
    "    frame = create_frame(env)\n",
    "\n",
    "    # On every step update the displayed observation\n",
    "    def on_step(observation, action, next_observation, reward, done, length, score):\n",
    "        update_frame(frame)\n",
    "\n",
    "    # Run a single observation\n",
    "    length, score, observations, actions, rewards = run_episode(env, policy, on_step)\n",
    "    \n",
    "    # Print the episode stats\n",
    "    print(f'Episode length: {length}, return: {score}')\n",
    "    return length, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to visually see the random policy in action.\n",
    "\n",
    "The environment is recreated here with explicitly setting the `render_mode` parameter to `rgb_array`. This makes it possible to show the output in an embedded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('Taxi-v3', render_mode='rgb_array')\n",
    "length, score = evaluate(env, random_policy)\n",
    "length, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's it. We implemented the functions that we are going to use in the next couple exercises."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
