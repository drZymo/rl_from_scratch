{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Actor-Critic\n",
    "\n",
    "Welcome to the last exercise in this series. We are going to do this one a little differently. In the previous chapter you implemented an agent with the DQN algorithm. In this chapter we are going to implement it using the Actor-Critic algorithm. But instead of building all the required methods step by step, we are going to take the previous implementation and refactor it to an Actor-Critic model.\n",
    "\n",
    "So, let's start again with importing some stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize some display libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_display\n",
    "init_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same Gym environment, `LunarLander-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env._max_episode_steps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor neural network\n",
    "\n",
    "The Actor part in this algorithm is a neural network that predicts the action probabilities of the policy. We use the already implemented `PolicyEstimator` class from ['estimator.py'](estimator.py). If you are interested and if you have time then you could take a look at the implementation.\n",
    "\n",
    "Let's see how that class works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimator import PolicyEstimator\n",
    "\n",
    "actor = PolicyEstimator(env)\n",
    "\n",
    "observation = env.reset()\n",
    "actor.predict(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should return a list of four non-zero probabilities (values between 0.0 and 1.0) that should all sum up to 1. This is the probability distribution of the policy for this state. Give it another state (i.e. observation), and it will different values. You can see this if you re-run this cell. The output should be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select action\n",
    "\n",
    "Instead of the $\\epsilon$-greedy policy, we are now going to select an action randomly according to the distribution given by the actor. So, actions with a higher probability will be selected more often.\n",
    "\n",
    "Below you see the **old** implementation of `select_action` of the previous exercise. You should refactor this to the new behavior.\n",
    "- Remove the `epsilon` parameter.\n",
    "- (optional) Rename parameter `estimator` to `actor`\n",
    "- Remove epsilon greedy part.\n",
    "- Predict action probabilities with `actor` instead of predicting action values\n",
    "- Select a random action using the given probability distribution. See [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) for more information. First parameter should be the number of actions to choose from, the and parameter `p` should be set to action probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, estimator, observation, epsilon):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Implement epsilon greedy algorithm.\n",
    "    # With probability `epsilon` take a random action,\n",
    "    # otherwise take the action with the highest probability.\n",
    "    if np.random.uniform() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action_values = estimator.predict(observation)\n",
    "        action = np.argmax(action_values)\n",
    "        \n",
    "    ### END CODE ###\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works. If you run the following cell, you should get a single integer between 0 and 4 as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "action = select_action(env, actor, observation)\n",
    "assert 0 <= action < 4\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic neural network\n",
    "\n",
    "The Critic in this algorithm is a neural network that estimates state values (i.e. $v(s)$). We will use the already implemented `StateValueEstimator` class from [`estimator.py`](estimator.py).\n",
    "\n",
    "Let's create one and see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimator import StateValueEstimator\n",
    "\n",
    "critic = StateValueEstimator(env)\n",
    "critic.load_weights('data/lunar-critic.h5')\n",
    "\n",
    "observation = env.reset()\n",
    "critic.predict(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that it returns only one value, which represents the expected return from this state, when following the learned policy. This will be trained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target values\n",
    "\n",
    "The previous algorithm (DQN) only produced one set of values needed for training, namely estimations for $q(s,a)$. However, for this algorithm we need two sets.\n",
    "- The advantages that are used as sample weights when training the actor.\n",
    "- The target values for $v(s)$ used to train the critic.\n",
    "\n",
    "Let's make two new functions to replace the `compute_target_values` function of the previous exercise.\n",
    "\n",
    "### Advantages\n",
    "Let's start with the advantages, and make a new function that computes the advantages as follows.\n",
    "\n",
    "$$\n",
    "A_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "$$\n",
    "\n",
    "In other words, compute the value of the current transition (reward + expected return of next state) and subtract the estimate of expected return of this state.\n",
    "\n",
    "If we received more reward than predicted, then the advantage will be positive and the actor should predict taking this action in this state more often. And of course, the actor should predict taking this action less often when the advantage is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(critic, observations, rewards, next_observations, dones, gamma):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Compute state values for both the current observations and the next observations\n",
    "    # using the critic estimator.\n",
    "    state_values = ...\n",
    "    next_state_values = ...\n",
    "    \n",
    "    # Compute the advantage\n",
    "    advantages = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted rewards\n",
    "\n",
    "For the critic we are going to use the return ($G_t$) of each state visited in this episode. Remember, the return of state $S_t$ is described as.\n",
    "\n",
    "$$\\begin{align}\n",
    "G_t &= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots \\\\\n",
    "    &= R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{align}$$\n",
    "\n",
    "We can easily compute the $G_t$ for each state in the trajectory of this episode, by traversing the rewards backwards. We start at $t = T$ and go backward to $t = 0$. Remember, $G_T$ is 0, i.e. the terminal state has no expected return.\n",
    "\n",
    "In python you can simply reverse a list using the built-in function `reversed(a)` and you can insert an item at the start of a list with `a.insert(0, v)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    discounted_rewards = []\n",
    "    ### START CODE ###\n",
    " \n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    return np.array(discounted_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works by running this small unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([0, 1, 2, 3, 0, -3])\n",
    "discount_rewards(rewards, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be exactly:\n",
    "\n",
    "    array([ 2.93553,  3.2617 ,  2.513  ,  0.57   , -2.7    , -3.     ])\n",
    "\n",
    "If this is not the case (besides floating point rounding errors), then check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "Now it is time to refactor the implementation of the `train` function from previous exercise.  This time the function will get the data of a complete trajectory in order (i.e. whole episode).\n",
    "\n",
    "Below is the implementation of the previous exercise. You have to refactor it. The basic structure is still the same. First step is to compute the target values, then use that data to train the estimators.\n",
    "- Rename parameter `estimator` to `actor`.\n",
    "- Add parameter `critic` (after `actor`).\n",
    "- Replace old `target_q` with:\n",
    "    - Compute the advantages using the function implemented above.\n",
    "    - Compute discounted rewards (i.e. returns) using the function implemented above.\n",
    "- Train both `actor` and `critic`:\n",
    "    - `actor` required `observations`, `actions` and `advantages`.\n",
    "    - `critic` required `observations` and `returns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(estimator, observations, actions, rewards, next_observations, dones, gamma):\n",
    "    ### START CODE ###\n",
    "\n",
    "    # Compute target values\n",
    "    target_q = compute_target_values(estimator, rewards, next_observations, dones, gamma)\n",
    "\n",
    "    # Train the estimator to this data.\n",
    "    # It needs both the `observations` and `actions` as input parameters to the neural network\n",
    "    # and it needs `target_q` as target for the output of the neural network\n",
    "    estimator.train_on_batch(observations, actions, target_q)\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test this implementation in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now, let's update the training procedure. First let's set some parameters. We won't be needing the `epsilon` and `batch_size` anymore, so they are left out. The other parameters remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "max_time = 300 # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, recreate the two estimators, so we can start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "...\n",
    "...\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay buffer\n",
    "\n",
    "In the DQN algorithm we used a buffer to store the gathered experience. For the Actor-Critic algorithm we work episode-by-episode, so we will reuse the implementation of the experience buffer here to store the trajectory. For this we will need two extra functions:\n",
    "- `experience_buffer.get_all()` to get all the data stored in the buffer in correct order.\n",
    "- `experience_buffer.reset()` clear the buffer, so a new episode can be stored.\n",
    "\n",
    "First, create a buffer object with 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ExperienceBuffer\n",
    "experience_buffer = ExperienceBuffer(1000, env.observation_space.shape, (1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop\n",
    "\n",
    "Finally it is time to update the training loop. Below is the implementation of the DQN training loop. You have to refactor this loop. The main things that need to change is the moment the training happens. Instead of every step we only train after each episode.\n",
    "\n",
    "1. Change the call to `select_action` according to the new implementation.\n",
    "2. Remove the train block from the inner loop.\n",
    "3. Add a train block after the episode is finished.\n",
    "    - Use `experience_buffer.get_all()` to get the trajectory of the episode.\n",
    "    - Call your new `train` function to train on the trajectory.\n",
    "    - Clear the buffer afterwards with the `experience_buffer.reset()` function.\n",
    "4. Store the weights of both estimators.\n",
    "    - Weights of actor should be stored in `lunar-actor.h5`.\n",
    "    - Weights of critic should be stored in `lunar-critic.h5`.\n",
    "    \n",
    "Good luck, you are almost there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "episode = 0\n",
    "while (datetime.now() - start_time).total_seconds() < max_time:\n",
    "    episode_length, episode_score = 0, 0\n",
    "\n",
    "    observation, done = env.reset(), False\n",
    "    while not done:\n",
    "        ### START CODE ###\n",
    "        \n",
    "        # Select an action using the epsilon-greedy policy\n",
    "        action = select_action(env, estimator, observation, epsilon)\n",
    "        \n",
    "        # Take action and get reward\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Add to experience buffer\n",
    "        experience_buffer.append(observation, action, reward, next_observation, done)\n",
    "        \n",
    "        ### END CODE ###\n",
    "\n",
    "        if experience_buffer.is_filled(batch_size):\n",
    "            ### START CODE ###\n",
    "            \n",
    "            # Select a random batch from experience\n",
    "            observations, actions, rewards, next_observations, dones = experience_buffer.sample(batch_size)\n",
    "\n",
    "            # Perform one train step on the batch\n",
    "            train(estimator, observations, actions, rewards, next_observations, dones, gamma)\n",
    "            \n",
    "            ### END CODE ###\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        observation = next_observation\n",
    "        episode_length += 1\n",
    "        episode_score += reward\n",
    "    \n",
    "    # Show statistics\n",
    "    t = (datetime.now() - start_time).total_seconds()\n",
    "    print(f'[{t:.1f}] #{episode}: length {episode_length}, score {episode_score:.0f}')\n",
    "\n",
    "    episode += 1\n",
    "\n",
    "# Save the end result\n",
    "estimator.save_weights('lunar.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the average score should have increased, but it could be more noisy this time.\n",
    "\n",
    "Time to evaluate the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluating the result is actually very similar as before. In the DQN exercise the evaluation loop selected the action with the highest value. With Actor-Critic we now have an actor that returns action probabilities. Now we only need to select the action with the highest probability. So the refactoring of this code is mainly cosmetic and is optional.\n",
    "\n",
    "- Rename `estimator` to `actor`.\n",
    "- Rename `action_values` to `action_probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_frame, update_frame\n",
    "\n",
    "def evaluate(estimator, env):\n",
    "    ### START CODE ###\n",
    "\n",
    "    # Reset environment\n",
    "    observation = env.reset()\n",
    "    \n",
    "    ### END CODE ###\n",
    "\n",
    "    done, score = False, 0\n",
    "    frame = create_frame(env)\n",
    "\n",
    "    while not done:\n",
    "        ### START CODE ###\n",
    "        \n",
    "        # Greedy select an action based on the estimated action values\n",
    "        action = select_action(env, estimator, observation, 0)\n",
    "        \n",
    "        # Perform action\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        \n",
    "        ### END CODE ###\n",
    "        score += reward\n",
    "        update_frame(frame)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the Actor-Critic method is that during training we need the Critic, but once trained we only need the Actor.\n",
    "\n",
    "Now, we can first evaluate the initial (random) policy. We only need an actor here. So let's create a new `PolicyEstimator` to start with a fresh, random neural network and evaluate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = PolicyEstimator(env)\n",
    "score = evaluate(actor, env)\n",
    "print(f'initial model: {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compare it with the actor that was trained by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_weights('lunar-actor.h5')\n",
    "score = evaluate(actor, env)\n",
    "print(f'partially trained model: {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs nowhere as good as it did during the DQN exercise. That is mainly cause by the different way of training. We now need more samples (i.e. episodes) to get the critic trained before it can steer the actor in the right direction. In the lecture you will see what can be done to improve this.\n",
    "\n",
    "Now finally, compare it with an actor that has been trained with an improved algorithm (PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_weights('trained/lunar-ppo-actor.h5')\n",
    "score = evaluate(actor, env)\n",
    "print(f'fully trained model: {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's it! You are done. You just transformed a DQN algorithm to an Actor-Critic algorithm. This model did not perform really well ... yet. It is, however, the starting point of much more efficient algorithms that will be discussed in the lectures.\n",
    "\n",
    "Well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
