{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/RabbitMDP.png\" width=\"50%\" align=\"right\" />\n",
    "\n",
    "# Policy iteration\n",
    "\n",
    "This notebook explains how the \"Policy iteration\" framework works.\n",
    "\n",
    "Let's first import the necessary components that were also used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import RabbitMDP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a new instance with which we can interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = RabbitMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "A policy determines the probability that an agent takes a certain action $a$ when the environment is in state $s$. It is denoted with the function:\n",
    "\n",
    "$$pi(a|s)$$\n",
    "\n",
    "A policy is stochastic, when in a state multiple actions have a non-zero probability, or deterministic when in each state only one action has a probability (of 1).\n",
    "\n",
    "In our example MDP the state-action space is small so we can put the entire policy in a simple dictionary. Let's define a completely random policy, where all actions allowed in a state have an equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {\n",
    "    'idle': {'wakeup': 1.0},\n",
    "    'hungry': {'go eat': 0.5, 'stay': 0.5},\n",
    "    'eating': { 'go eat': 0.5, 'go home': 0.5},\n",
    "    'dead': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's walk through an trajectory and see how the agent interacts with the environment.\n",
    "The environment starts in the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idle'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = mdp.STATES[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this state there is only one action allowed, and the policy should give that action a probability of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wakeup': 1.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the agent would take this action. What are the possible transitions the environment could take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'wakeup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungry, 0: 1\n"
     ]
    }
   ],
   "source": [
    "for s_next in mdp.STATES:\n",
    "    for r in mdp.REWARDS:\n",
    "        p = mdp.p(s, a, s_next, r)\n",
    "        if p > 0: print(f'{s_next}, {r}: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one transition, with probability 1, that will put the environment in state `hungry`.\n",
    "\n",
    "In this state the agent should take the next action. The policy now returns the following probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go eat': 0.5, 'stay': 0.5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'hungry'\n",
    "pi[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete random choice between two actions. We could take one action and see what happens, but let's continue with the next step. Evaluating this policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is a policy? That can simply be determined by computing the value of each state, and in particular the start states. The higher the state-values for the first states, the higher the expected return when following this policy.\n",
    "\n",
    "We don't know the values yet, so let's start by initializing them to 0 and see where we get from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0, 'hungry': 0, 'eating': 0, 'dead': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = {state: 0 for state in mdp.STATES}\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action value function $q$\n",
    "\n",
    "We have seen the computation of action values in the previous notebook. With the following function we can compute the value of taking action $a$ when in state $s$.\n",
    "\n",
    "$$ q(s,a) = \\sum_{s'} \\sum_r p(s',r|s,a)[r + \\gamma \\mathop{v_\\pi}(s')] $$\n",
    "\n",
    "Let's define a Python function that computes this using the table if state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(s, a, V):\n",
    "    value = 0\n",
    "    for s_next in mdp.STATES:\n",
    "        for r in mdp.REWARDS:\n",
    "            p = mdp.p(s, a, s_next, r)            \n",
    "            value += p * (r + gamma * V[s_next])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses $\\gamma$ as a discount factor, so let's define that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with the initial state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q('idle', 'wakeup', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns 0, because the action does not give a reward and the value of the next state is still 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6000000000000001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q('hungry', 'go eat', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns 0.6, because the `go eat` action can result in two transitions one with probability 0.8 and reward 1 and one with probability 0.2 and reward -1. Both end up in a state we don't have a value for, so we get the following computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6000000000000001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8 * (1 + gamma * 0) + 0.2 * (-1 + gamma * 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q('hungry', 'stay', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State value function $v$\n",
    "\n",
    "With this action value function $q$ we can compute the value of a state, by inspecting all actions that are allowed in a state.\n",
    "\n",
    "$$ v(s) = \\sum_a \\mathop{\\pi}(a|s) q(s,a) $$\n",
    "\n",
    "Let's implement this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v(s, pi, V):\n",
    "    value = 0\n",
    "    for a in mdp.A(s):\n",
    "        value += pi[s][a] * q(s, a, V)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check to so what happens in the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v('idle', pi, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is again 0, because only 1 action is allowed, and that results in no reward, to a state with no value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25000000000000006"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v('hungry', pi, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This state has a value because two actions are possible both with a reward.\n",
    "\n",
    "`go eat` has an action value of 0.6 and `stay` has an action value of -0.1. Both actions have equal probability of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * 0.6 + 0.5 * -0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the state values for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0.0, 'hungry': 0.25000000000000006, 'eating': -0.1, 'dead': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_new = {s: v(s, pi, V) for s in mdp.STATES}\n",
    "V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation convergence\n",
    "\n",
    "We now have a table of values, let's see what happens if we use these values in our computations of $q$ and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0, 'hungry': 0, 'eating': 0, 'dead': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = {s: 0 for s in mdp.STATES}\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0.22500000000000006,\n",
       " 'hungry': 0.31525000000000003,\n",
       " 'eating': -0.1225,\n",
       " 'dead': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = V_new\n",
    "V_new = {s: v(s, pi, V) for s in mdp.STATES}\n",
    "V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values have changed again, but with a smaller amount. Let's see if we iterate 150 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0.41213695806784034,\n",
       " 'hungry': 0.4579299534087115,\n",
       " 'eating': 0.06241200632828714,\n",
       " 'dead': 0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(150):\n",
    "    V_new = {s: v(s, pi, V) for s in mdp.STATES}\n",
    "    V = V_new\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we run it one more time we see that it doesn't change anymore. The values have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0.41213695806784034,\n",
       " 'hungry': 0.4579299534087115,\n",
       " 'eating': 0.06241200632828714,\n",
       " 'dead': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_new = {s: v(s, pi, V) for s in mdp.STATES}\n",
    "V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to create a function that keeps computing until the maximum difference is below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pi):\n",
    "    # initialize the values of all states to 0\n",
    "    V = {s: 0 for s in mdp.STATES}\n",
    "    while True:\n",
    "        # compute new state values\n",
    "        V_new = {s: v(s, pi, V) for s in mdp.STATES}\n",
    "        # compare with previous values\n",
    "        diff = np.abs([V_new[s] - V[s] for s in mdp.STATES]).max()\n",
    "        V = V_new\n",
    "        # stop when difference is below threshold\n",
    "        if diff < 1e-9:\n",
    "            break\n",
    "    return V   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should return the same values as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0.41213695568355024,\n",
       " 'hungry': 0.4579299514857902,\n",
       " 'eating': 0.062412004614500693,\n",
       " 'dead': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = evaluate(pi)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table now contains, for each state, the expected return when starting in this state and following policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement\n",
    "\n",
    "Now that we have accurate the state values (and therefore action values), we can improve our policy. We know, in each state, which actions result in the highest expected return. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idle\n",
      "   wakeup 0.4121369563372112\n",
      "hungry\n",
      "   go eat 0.6449366433224406\n",
      "   stay 0.27092326070349004\n",
      "eating\n",
      "   go eat 0.028085402076525323\n",
      "   go home 0.09673860809215618\n",
      "dead\n"
     ]
    }
   ],
   "source": [
    "for s in mdp.STATES:\n",
    "    print(s)\n",
    "    for a in mdp.A(s):\n",
    "        print('  ', a, q(s, a, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that in state `hungry` the action `go eat` has a higher state-action value, and in state `eating` the action `go home` has a higher value.\n",
    "\n",
    "So, if we want to maximize the expected return, i.e. get a better policy, we should change the probabilities so the actions with higher values are chosen. We can do this by acting greedy on the state values. Let's always take the action with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': {'wakeup': 1.0},\n",
       " 'hungry': {'go eat': 1, 'stay': 0},\n",
       " 'eating': {'go eat': 0, 'go home': 1},\n",
       " 'dead': {}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi['hungry']['go eat'] = 1\n",
    "pi['hungry']['stay'] = 0\n",
    "pi['eating']['go eat'] = 0\n",
    "pi['eating']['go home'] = 1\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now our state values are no longer correct. They represent the values when following the previous policy. This policy is no longer the same, so we should re-compute our state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idle': 0.7693461298894951,\n",
       " 'hungry': 0.8548290332105501,\n",
       " 'eating': 0.3539292137503331,\n",
       " 'dead': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = evaluate(pi)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous policy the values have changed a lot. Let's take a look at the action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idle\n",
      "   wakeup 0.7693461298894951\n",
      "hungry\n",
      "   go eat 0.85482903390024\n",
      "   stay 0.5924115169005456\n",
      "eating\n",
      "   go eat 0.15926814618764995\n",
      "   go home 0.35392921352043644\n",
      "dead\n"
     ]
    }
   ],
   "source": [
    "for s in mdp.STATES:\n",
    "    print(s)\n",
    "    for a in mdp.A(s):\n",
    "        print('  ', a, q(s,a, V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have also changed a lot absolutely. But for each state, the action with the highest value has not changed.\n",
    "\n",
    "If we want to get the highest expected return, then we should still act greedy and that means, no change to the current policy.\n",
    "\n",
    "We are done, our policy is the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy with NumPy\n",
    "\n",
    "How can you act greedy on action values programmatically?\n",
    "\n",
    "Let's see for a single state what the action values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go eat': 0.85482903390024, 'stay': 0.5924115169005456}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'hungry'\n",
    "{a: q(s, a, V) for a in mdp.A(s)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only pick the values, and not the state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.85482903390024, 0.5924115169005456]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q(s,a,V) for a in mdp.A(s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With numpy we can see what the maximum value of this list is using `np.max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85482903390024"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([q(s,a,V) for a in mdp.A(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, with `np.argmax` we can find out the index in the list where the maximum value is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_a = np.argmax([q(s,a,V) for a in mdp.A(s)])\n",
    "max_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works in another state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'eating'\n",
    "max_a = np.argmax([q(s,a,V) for a in mdp.A(s)])\n",
    "max_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which action is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go home'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.A(s)[max_a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what is the value of that action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35392921352043644"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([q(s,a,V) for a in mdp.A(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our policy is greedy, i.e. is taking the action with the highest expected return, the value of this state should be the value of taking this action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3539292137503331"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V[s]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e19a0a336d27966aafd7dca68dcda35a2885f4385a8657b9af38aaac43db0040"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
