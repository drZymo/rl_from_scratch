{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Policy Gradient\n",
    "\n",
    "Welcome to the fourth and final exercise of this series. We will now train an agent that uses a neural network to predict the action probabilities directly, i.e. the policy function $\\pi$, instead of the action values. This will reduce the number of intermediate steps and makes the training process simpler.\n",
    "\n",
    "As usual we have to import a few components first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from policy_gradient import PongEnv\n",
    "from utils import Episode\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/atari_2600.jpg\" width=\"25%\" align=\"right\" /> For this final exercise we will take another step up and use an even more complex environment. Time for some actions! We will learn to play the game Pong for the Atari 2600. The OpenAI Gym library has an [Atari section](https://gym.openai.com/envs/#atari) with a lot of those classic games.\n",
    "\n",
    "As explained in the lectures, the observation space of the default Pong game are too big for this exercise (210,160,3). It will take a lot of time and memory to train on this, so we have simplified the observations a little. We have wrapped the `PongNoFrameskip-v4` environment in the class `PongEnv` that reduces the observation size to (80,80), i.e. black and white, and automatically stacks two frames together, so you can determine the direction of the ball.\n",
    "\n",
    "Let's create it first, and then inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PongEnv()\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As told, the observation is two frames of 80x80 pixels. In other words, 12800 values. A lot more than the 8 values of the LunarLander environment used in the previous exercise.\n",
    " \n",
    "The action space is still a discrete integer. This time 6 actions are supported, which are:\n",
    "`NOOP`, `FIRE`, `RIGHT`, `LEFT`, `RIGHTFIRE`, `LEFTFIRE`. In other words, a joystick that can move left or or right and a fire button that can be pressed simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at those black and white images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "observation, _, _ ,_ = env.step(4)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "ax1.imshow(observation[:,:,0], cmap='gray')\n",
    "ax2.imshow(observation[:,:,1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the left (current) frame that the ball has moved compared to the right (previous) frame, as well as our paddle on the right side. That's the whole reason for having two frames. You can see movement. Otherwise it would be impossible to determine which way the ball is going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Like in the previous exercise we will be using TensorFlow to create a neural network. We will use the same [Keras Functional API](https://www.tensorflow.org/guide/keras/functional) as used in the previous exercise.\n",
    "\n",
    "So, let's import the necessary TensorFlow components again. (Eager execution is again disabled to increase performance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Disable eager execution for performance\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction model\n",
    "\n",
    "<img src=\"figures/pong-net.png\" width=\"50%\" align=\"right\"/> Like before we will first build a model that we can use for predicting. This time we will have to predict the action probabilities directly. We will build the neural network as described during the lecture, and depicted on the right.\n",
    "\n",
    "This means we will first build to [`Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers, with the right values for the `filters`, `kernel_size`, `strides`, `padding` and `activation` parameters. You should leave the other parameters at their default values.\n",
    "\n",
    "Then [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) the intermediate tensor to a single dimension, so we can use it in the next [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers. The `Flatten` class does not have any parameters except for its name.\n",
    "\n",
    "Finally, a [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) should be constructed with the righ in and outputs. Then we have a model that can predict the action probabilities, i.e. the policy.\n",
    "\n",
    "Be sure to set the `name` parameter on all layers to make it easier to check the summary later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "load_model('trained/pong.h5').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ### START CODE ###\n",
    "    # Create an Input layer with shape equal to the observation shape of the environment\n",
    "    observation = ...\n",
    "    \n",
    "    # Create the two convolution layers of the depicted network\n",
    "    # Use a padding parameter equal to 'valid'.\n",
    "    conv1 = ...\n",
    "    conv2 = ...\n",
    "    \n",
    "    # Create a layer to flatten the output of the convolution layers to a single dimension\n",
    "    flatten = ...\n",
    "    \n",
    "    # Create one hidden dense layer\n",
    "    dense = ...\n",
    "    \n",
    "    # Create the final dense layer that outputs the action probabilities.\n",
    "    # Be sure to use the 'softmax' activation function!\n",
    "    action_probs = ...\n",
    "    \n",
    "    # Create model that uses the observation layer as inputs and the action probabilites as outputs.\n",
    "    model = ...\n",
    "    ### END CODE ###\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your summary should look similar to:\n",
    "\n",
    "    Model: \"pong\"\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    observation (InputLayer)     [(None, 80, 80, 2)]       0         \n",
    "    _________________________________________________________________\n",
    "    conv1 (Conv2D)               (None, 19, 19, 32)        3168      \n",
    "    _________________________________________________________________\n",
    "    conv2 (Conv2D)               (None, 4, 4, 64)          100416    \n",
    "    _________________________________________________________________\n",
    "    flatten (Flatten)            (None, 1024)              0         \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, 128)               131200    \n",
    "    _________________________________________________________________\n",
    "    action_probs (Dense)         (None, 6)                 774       \n",
    "    =================================================================\n",
    "    Total params: 235,558\n",
    "    Trainable params: 235,558\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________\n",
    "\n",
    "Check the number of layers and the number of parameters. This should be exactly equal. Names could be different.\n",
    "\n",
    "If this is not correct, then check if you passed the right layers as inputs to the other layers. Did you hook them up in the right order?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "The nice thing about this policy gradient method is that we actually don't need a different training model. TensorFlow has all the right built-in functionality that we can use to train the model with additional `Lambda` layers. The prediction model predicts probabilities for all the actions. But as explained in the lecture, we can use the cross-entropy loss in combination with the one-hot vectors of the selected actions to obtain the action probability of only the selected action. In TF this loss function is already registered with the name `'categorical_crossentropy'` which we can use. (In the previous exercise we used `'mse'` for that). However, we still need a way to pass along the returns ($G_t$), but we'll come to that later.\n",
    "\n",
    "We will also be using an instance of the `Adam` optimizer with the given learning rate.\n",
    "\n",
    "So, all we need to do is to [`compile`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) the model with the right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate):\n",
    "    ### START CODE ###\n",
    "    # compile the model with the right loss and optimizer parameter.\n",
    "    ...\n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy that will be used for interacting with the environment will be a bit different than before. We will no longer use an $\\epsilon$-greedy policy. This time we will simply pick actions at random, using the probability distribution $\\pi$ that the model outputs.\n",
    "\n",
    "Let's first take a look at that output. Run this cell to see what the output of the model looks like. Note that the observation is again reshaped to a batch of 1 using the `expand_dims` function.\n",
    "\n",
    "(Ignore the potential deprecation warnings you can get here. Run the cell another time to get rid of them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "observation = env.reset()\n",
    "action_probs = model.predict_on_batch(np.expand_dims(observation, axis=0))[0]\n",
    "print(f'pi(s) = {action_probs}')\n",
    "print(f'sum = {action_probs.sum():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model returns 6 numbers close to the 0.166667 we expect for a completely random policy. This means the initial weights of the model are such that the input of the softmax is similar for all the actions, and therefore the probability for all actions is similar. If you run the cell multiple times you will see the values change, but not by much.\n",
    "\n",
    "All 6 probabilities should sum up to 1 (with some floating point arithmetic errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the policy in the following function.\n",
    "\n",
    "First we have to predict the action probabilities (the policy) using the model. [`predict_on_batch`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_on_batch) will require a batch as input, so use [`np.expand_dims`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html) with `axis=0` to make it a batch of 1. Then take the first item of the list that `predict_on_batch` returns to convert the batch of 1 back to a single item.\n",
    "\n",
    "NumPy has a nice function for randomly selecting a value using a given probability distribution, called [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html). Its first parameter can simply be an integer (e.g. `6` or `len(probs)`), making it select a value between 0 and this value. The parameter `p` can be used to give it a probability distribution, which should have a length as given by the first parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, observation):\n",
    "    ### START CODE ###\n",
    "    action_probs = ...\n",
    "    action = ...\n",
    "    ### END CODE ###\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode\n",
    "\n",
    "The algorithm we are using (REINFORCE) is a Monte Carlo or episodic algorithm. That means we only train once the entire episode is finished. So let's make a function that runs a complete episode and stores all required data.\n",
    "\n",
    "We will use the helper class `Episode` to store the data, which is implemented in the [utils.py](utils.py) file. Take a look if you are interested. It has a constructor that needs a `capacity`, `observation_shape` and `action_shape` parameters.\n",
    "\n",
    "For capacity we we will use `2000`. This means we will store at most 2000 observations. This is enough for our case because we won't train it beyond a certain level. A buffer of 2000 observation still requires at least `2000*80*80*2*sizeof(float32) = 98 MB` of memory. If we would have used the raw observations (2 frames stacked), then it would have needed `2000 * 210*160*3 * 2 * sizeof(float32) = 1.5 GB`. With 10+ simultaneous users that would have been a bit too much for this server.\n",
    "\n",
    "Furthermore, it has a function `append` that requires an observation, action and reward, which it will store. The action should be a one-hot vector. You can use the (already imported) function [`to_categorical`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) for that. It only requires a `num_classes` parameter which should be `6` in this case.\n",
    " \n",
    "So implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, model):\n",
    "    episode = Episode(2000, (80,80,2), (6,))\n",
    "\n",
    "    ### START CODE ###\n",
    "    # Reset the environment\n",
    "    observation = ...\n",
    "    done = False\n",
    "\n",
    "    # Loop until the episode is finished\n",
    "    ...:\n",
    "        # Select an action for the current observation using the model\n",
    "        action = ...\n",
    "\n",
    "        # Take an environment step with that action.\n",
    "        next_observation, reward, done, _ = ...\n",
    "\n",
    "        # Convert action to a one-hot vector\n",
    "        action = ...\n",
    "        \n",
    "        # Append the observation, action and reward to the episode\n",
    "        ...\n",
    "\n",
    "        # Make the next observation the current observation\n",
    "        observation = next_observation\n",
    "\n",
    "    ### END CODE ###\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you implemented it correctly by running one episode and plotting the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "episode = run_episode(env, model)\n",
    "\n",
    "observations, actions, rewards = episode.get()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the moments in time that it receives a reward by the spikes in the graph. Not all rewards are received at the same interval, sometimes it takes a bit longer. Most of them, if not all, rewards are -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected return\n",
    "\n",
    "We will not be using a value estimation function in this algorithm, so we have to compute the expected return ($G_t$) ourselves. Remember from the lectures that this is defined iteratively as follows.\n",
    "\n",
    "$$ G_t = R_t + \\gamma G_{t+1} $$\n",
    "\n",
    "That means that if you go backwards from the end of the episode to the front then you can compute it easily. Let's do that in the function we call `discount_rewards`. It receives the rewards array gathered during an episode and the discount rate `gamma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma):\n",
    "    returns = np.zeros_like(rewards)\n",
    "    ### START CODE ###\n",
    "    # Initialize current return.\n",
    "    # This is the expected return for the terminal state, i.e. G_T.\n",
    "    current_return = ...\n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Go in reverse\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        ### START CODE ###\n",
    "        # Compute the return at time t, using\n",
    "        # the \"previous\" return of (t+1) and the current reward.\n",
    "        current_return = ...\n",
    "        ### END CODE ###\n",
    "        returns[t] = current_return\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the effect of this discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = discount_rewards(rewards, 0.9)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now clearly see that the spikes have longer tails to the left of them. This means that the expected return of an action taken at a time before a spike (i.e. a reward) will also be regarded positive or negative. This will effectively give a reward to an action proportional to how good the future will be. It is an estimated of the expected value of that action; an estimate of $q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the final piece missing is the training step. This time we will fit the model to the data of an entire episode, instead of a random batch of experience.\n",
    "\n",
    "We will first have to convert the rewards of the episode to the expected returns ($G_t$). Use the just implemented `discount_rewards` function.\n",
    "\n",
    "As promised, we still have to explain how to get the returns into the training procedure. As explained in the lecture, the loss function we can use that implements the policy gradient method looks like:\n",
    "\n",
    "$$ L(\\theta) = G \\mathop{\\text{CrossEntropy}} ({\\pi_\\theta}(S), A) $$\n",
    "\n",
    "The [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) function of the model, which we used before, has a nice parameter called `sample_weight`. It will use this to scale the loss function for all the samples in the batch. It will multiply the losses (cross-entropy in this case) with the corresponding scalar value in the `sample_weight` input. So if we use `returns` as sample weights, then we get exactly what we needed.\n",
    "\n",
    "Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, episode, gamma):\n",
    "    # Unpack the episode\n",
    "    observations, actions, rewards = episode.get()\n",
    "\n",
    "    ### START CODE ###\n",
    "    # Compute the expected returns using the discount function\n",
    "    returns = ...\n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Normalize the returns to make training more efficient\n",
    "    returns -= returns.mean()\n",
    "    returns /= returns.std()\n",
    "\n",
    "    ### START CODE ###\n",
    "    # Fit the model to the observations and use actions as target to compute the cross-entropy loss.\n",
    "    # Scale the losses for each same using returns as sample_weights.\n",
    "    # Run for only 1 epoch (epochs=1), and turn off logging with verbose=0.\n",
    "    ...\n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Time for the actual training. We start with a number of hyperparameters that we can change if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_episodes = 250\n",
    "learning_rate = 3e-5\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and compile a fresh new model that is initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "compile_model(model, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's implement the loop itself. It is actually very short. We run an entire episode with the model, train on that episode, and repeat.\n",
    "\n",
    "We will keep track of the individual scores of the episodes so we can nicely plot them when we are done.\n",
    "\n",
    "**Training will again take about 15 minutes.** Take a break, and come back to see what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "for e in range(1, nr_episodes+1):\n",
    "    ### START CODE ###\n",
    "    # Run an episode\n",
    "    episode = ...\n",
    "    # Train on episode\n",
    "    ...\n",
    "    ### END CODE ###\n",
    "\n",
    "    # Keep track of score\n",
    "    scores.append(episode.score)\n",
    "    # Show statistics\n",
    "    delta = (datetime.now() - start_time).total_seconds()\n",
    "    print(f'[{delta:.1f}] #{e}: length {episode.length}, score {episode.score:.0f}')\n",
    "\n",
    "# Save trained model\n",
    "model.save('pong.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this took a long time. Take a look at the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It barely improved! How can this be? There are multiple reasons, but the main reason is the REINFORCE algorithm we used. The lecture will explain it in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the training didn't look promising, we will still take a look at how well it performs now. Like before we will use an evaluation function to run an entire episode, display the frames, and return the total score. (Already implemented; simply run the cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_frame, update_frame\n",
    "\n",
    "def evaluate(env, model):\n",
    "    # Reset environment\n",
    "    observation, done, score = env.reset(), False, 0\n",
    "\n",
    "    # Setup display for first frame\n",
    "    frame = create_frame(env)\n",
    "\n",
    "    while not done:\n",
    "        # Predict action probabilities\n",
    "        action_probs = model.predict_on_batch(np.expand_dims(observation, axis=0))[0]\n",
    "        # Select action with highest probability\n",
    "        action = np.argmax(action_probs)\n",
    "        # Perform action\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        # Update displayed frame\n",
    "        update_frame(frame)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the initial, random model behaves, which is actually not that bad. It regularly hits the ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(env, build_model())\n",
    "print(f'score {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the model we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "score = evaluate(env, load_model('pong.h5'))\n",
    "print(f'score {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't do well. You can it twitch, but it fails miserably. That is mainly caused by the lack of training. It uses the `argmax` function, but that's not the right choice yet. More training is needed.\n",
    "\n",
    "Let's take a look how this neural network (i.e. model) can perform when trained with a proper algorithm (PPO) for a longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(env, load_model('trained/pong.h5'))\n",
    "print(f'score {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really owns the game. It learned the neat trick to hit the ball in such a way that the game's AI isn't fast enough to catch it, and it repeats this trick constantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish\n",
    "\n",
    "This marks the end of the exercises. You have now learned to implement all the basic reinforcement algorithms.\n",
    "\n",
    "This last one, Policy Gradient, is actually quite simple to implement, but it is really hard to get it working right. There are some simple improvements that you can use to get it working a lot better, which will be discussed in the lectures.\n",
    "\n",
    "We hope you enjoyed it. Have fun applying your newly acquired knowledge in other, more engaging environments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
