{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Q Network\n",
    "\n",
    "Welcome to the third exercise in this series. In the previous exercises we worked with relatively simple environments that were solvable quickly because the number of states were limited. In this exercise we will create an agent that learns a policy for an environment with lots of states. We will still be estimating the action values, but instead of storing them in a table we will be approximating them with a neural network.\n",
    "\n",
    "This exercise is going to be big compared to the others, so buckle up!\n",
    "\n",
    "Let's first include a number of necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from utils import ExperienceBuffer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in this notebook we want to display a few screens to you, but when you run this on a server there is no display available, so we need to start a virtual display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyvirtualdisplay import Display\n",
    "    disp = Display(visible=False, size=(400, 300)).start()\n",
    "except:\n",
    "    print('Virtual display not used')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar lander\n",
    "\n",
    "We will be using another Gym environment, this time we will use the `LunarLander` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env._max_episode_steps = 500\n",
    "\n",
    "print(f'Observation: {env.observation_space}, {env.observation_space.dtype}')\n",
    "print(f'Action:      {env.action_space}, {env.action_space.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the observation/state of this environment is a `Box`, which basically means a multi-dimensional array. This time with one dimension (i.e. it is a vector) of 8 floating point values instead of a single integer. This means that the number of combinations of values is near infinite and not feasible to estimate with a table. We will have to use function approximation.\n",
    "\n",
    "The action space is (like before) an integer with 4 discrete values.\n",
    "\n",
    "**Note**: the change in maximum episode steps is done to improve training speed in this exercise. With the default of 1000 steps the training time would be significantly longer and we don't have time for that. It won't affect the end result too much.\n",
    "\n",
    "We can render the current observation to an image to get a better feel for what the environment looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "print(observation)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there is a moon lander on the top of the screen and a landing zone marked with two flags.\n",
    "\n",
    "The goal of our agent is to land the lander on the target zone. The lander has 3 thrusters, one vertical (downwards) and two horizontal to the left and right. The agent has 4 actions, one for firing each thruster and one action to do nothing. So there is at most one thruster active at all times.\n",
    "\n",
    "If it lands too fast, it crashes and you'll get a negative reward. If it lands safely you get a positive reward. For all the fuel you spend you will get a penalty as well. But we don't have a model, so we pretend we don't know this ;).\n",
    "\n",
    "The observation you get is a list of distances and velocities compared to the ground. It is not really important which value is what, because our agent will figure that out by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "<img src=\"figures/dqn architecture.png\" align=\"right\" width=\"33%\" /> We want to implement a function that can approximate the action values for each observation, so we can implement a greedy policy. We will implement this with a neural network and we will use [TensorFlow](https://www.tensorflow.org/) (version 2.3) for this.\n",
    "\n",
    "TensorFlow is an open-source platform for machine learning and is, together with PyTorch, the most popular. It has a nice layered API that allow you to work with the bare computational graphs up to pretrained models. We will be using the higher level [Keras Functional API](https://www.tensorflow.org/guide/keras/functional), which is the recommended way of working with TensorFlow (since 2.0).\n",
    "\n",
    "The network architecture that we are going to use is as depicted on the right. It will consist of an input layer with the size of the observation (8), then 3 hidden layers with decreasing size, and finally an output layer that gives the values for each action (4). In TensorFlow these will be combined into a model, which can be used for inference (i.e. predict values) and training. Let's start by making the model architecture first and worry about training it later.\n",
    "\n",
    "First include the necessary TensorFlow components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Disable eager execution for performance\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [Eager Execution](https://www.tensorflow.org/guide/eager) is a handy feature of TensorFlow during development, but it also reduces the performance significantly, so we turn it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction model\n",
    "\n",
    "Now we will build the model needed for predicting/approximating action values.\n",
    "\n",
    "The Keras Functional API works by creating an instance of a layer class and then calling (i.e. invoking) that instance with other layers as parameters. The functional API means that the instances are callable functions. Finally, the input layer and the output layer of your neural network have to be passed to the constructor of the `Model` class in order to create a TensorFlow model that can be used for inference and training.\n",
    "\n",
    "In pseudo-code this looks like:\n",
    "    \n",
    "    i = Input(...)\n",
    "    h = Dense(...)(i)\n",
    "    o = Dense(...)(h)\n",
    "    m = Model(i, o)\n",
    "\n",
    "In this exercise you will only need to use the layer classes [`Input`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input) and [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), and combine them in a model with the [`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) class. Be sure to set the parameter `name` for all the layers, this will make it easier to see the structure later on.\n",
    "\n",
    "    Dense(..., name='hidden2')\n",
    "\n",
    "Furthermore, all hidden layers should use `activation` parameter `'tanh'` and the output layer should use `activation` parameter `'linear'` because we want to predict a value with no range limitation.\n",
    "\n",
    "Let's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction_model():\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Create an Input layer with shape (8,), i.e. the shape of the observation\n",
    "    # It doesn't required any other layers as input\n",
    "    observation = ...\n",
    "    \n",
    "    # Create three Dense layers with the appropriate sizes.\n",
    "    # All should use the activation function 'tanh'.\n",
    "    # Be extra careful to pass the right input objects to the layers.\n",
    "    dense1 = ...\n",
    "    dense2 = ...\n",
    "    dense3 = ...\n",
    "    \n",
    "    # Create on final Dense layer that will be the output layer.\n",
    "    # Its size should be the number of actions.\n",
    "    # Activation function is 'linear'\n",
    "    action_values = ...\n",
    "    \n",
    "    # Finally create a model with the input layer for the 'inputs' parameter\n",
    "    # and the last layer for the 'outputs' parameter\n",
    "    model = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if you implemented it correctly. If you run the next cell it will create a model and show a summary of the structure. The `name` parameters you used will be visible here, making it easier to check if everything is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = build_prediction_model()\n",
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should look similar to:\n",
    "\n",
    "    Model: \"lunar-dqn\"\n",
    "    _________________________________________________________________\n",
    "    Layer (type)                 Output Shape              Param #   \n",
    "    =================================================================\n",
    "    observation (InputLayer)     [(None, 8)]               0         \n",
    "    _________________________________________________________________\n",
    "    dense1 (Dense)               (None, 256)               2304      \n",
    "    _________________________________________________________________\n",
    "    dense2 (Dense)               (None, 128)               32896     \n",
    "    _________________________________________________________________\n",
    "    dense3 (Dense)               (None, 64)                8256      \n",
    "    _________________________________________________________________\n",
    "    action_values (Dense)        (None, 4)                 260       \n",
    "    =================================================================\n",
    "    Total params: 43,716\n",
    "    Trainable params: 43,716\n",
    "    Non-trainable params: 0\n",
    "    _________________________________________________________________\n",
    "\n",
    "Compare it carefully, the total number of parameters should match exactly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting\n",
    "With this model we can do predictions using the [`predict_on_batch`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_on_batch) function. The model is initialized randomly so it won't predict anything useful, but at least we can see it work.\n",
    "\n",
    "TensorFlow models are designed to be used with batches of data. So if you want to use it with a single element (i.e. one observation), you have to change the shape of the input data. In this case we have an observation of shape `(8,)` but the model requires something of the shape `(?, 8)`. This can be done simply by using the function [`np.expand_dims`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html), with `axis=0`. This will transform our observation from `(8,)` to `(1,8)`.\n",
    "\n",
    "**Note!** TensorFlow development is still in the middle of transferring all APIs from the v1 core to the v2 core. So you can get some warnings about deprecated functions. You can safely ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "\n",
    "# Reset the environment, to get the initial observation\n",
    "observation = ...\n",
    "print(f'observation shape was {observation.shape}')\n",
    "\n",
    "# Reshape the observation to a single batch\n",
    "observation = ...\n",
    "print(f'observation shape is now {observation.shape}')\n",
    "\n",
    "# Call the function `predict_on_batch` with the observation as input to get a prediction for the action values\n",
    "action_values = ...\n",
    "\n",
    "### END CODE ###\n",
    "action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now see and array with 4 action values. They don't make any sense yet, because we haven't trained the model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectors\n",
    "\n",
    "As explained during the lecture, and similar to the previous exercise, we want to update the approximations for every interaction the agent makes with the environment. This means that for every item in our training set we will have an action value for only one action, not for all 4. So far, our model outputs 4 values, and will need 4 values as target values when training.\n",
    "\n",
    "So to be able to train our model, we will have to extend our model to only output the value of a selected action.\n",
    "\n",
    "There is no built-in layer in TensorFlow that can do this, but we can simply implement it ourselves with a [`Lambda`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input) layer. This is a layer which accepts a function (e.g. a python `lambda` function) that is executed for every input the layer gets.\n",
    "\n",
    "A simple way to get the action value for a single action is by multiplying the output of the prediction model with the so-called one-hot vector of the action selected. A one-hot vector is a vector with all zeroes except for one item which is set to one. This can be used to convert integers to vectors and vice-versa. The helper function [`to_categorical`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) can do this for you. This multiplication will result in a vector with all zeroes, except for the action you selected. If you take the sum of this row, then you get only the value for the selected action.\n",
    "\n",
    "Let's take a look at the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 2\n",
    "action_onehot = to_categorical(action, num_classes=4)\n",
    "print(f'action_onehot = {action_onehot}')\n",
    "print(f'action_values = {action_values}')\n",
    "\n",
    "# Multiply the action values with the one hot vector to mask out all other values.\n",
    "action_value = action_values * action_onehot\n",
    "print(f'action_value = {action_value}')\n",
    "\n",
    "# Sum the columns (axis=1) to get only the selected action value.\n",
    "# Note: the keepdims parameter will make sure the dimensions don't change from (N,4) to (N,), but stay (N,1),\n",
    "# which is needed for the next layers in the model.\n",
    "action_value = np.sum(action_value, axis=1, keepdims=True)\n",
    "print(f'action value for selected action is {action_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model\n",
    "This we will use in our `Lambda` layer. Be aware that the `Lambda` layer should use TensorFlow functions instead of numpy functions, otherwise the computation graph that TensorFlow builds internally will not be correct. Luckily, TF includes alternatives to a lot of numpy functions in its `tf.keras.backend` package which we included with name `K`. So, the `np.sum` function is available to you as [`K.sum`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/sum) and behaves the same as in aboves example.\n",
    "\n",
    "So next to the observation our training model will also need the action for which we want to train the value. This is a new `Input` layer.\n",
    "\n",
    "The nice thing about the Keras Functional API is that you can use another model inside your model, so we can simply call the previous model with our observation input and get all the action values as predicted by that model. TensorFlow will automatically include this sub-model in it's internal computation graph, so the weights of this included sub-model are also updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_model(model_predict):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Create two input layers.\n",
    "    # one for the observation (with the same shape as before) and \n",
    "    observation = ...\n",
    "    # one for the one-hot action (i.e. shape (4,))\n",
    "    action = ...\n",
    "\n",
    "    # Call the prediction model object as if it is a layer instance with the observation as input to get all action values\n",
    "    action_values = ...\n",
    "    \n",
    "    # Implement a Lambda layer similar to the numpy example above.\n",
    "    action_value = Lambda(lambda x: ..., name='action_value')([action_values, action])\n",
    "    \n",
    "    # Create a new Model instance with BOTH the observation and action as input.\n",
    "    # Use an array ([]) for this.\n",
    "    # Use the output of the lambda layer as output to this model.\n",
    "    model_train = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation\n",
    "\n",
    "Finally, all we have to do now is a so-called compilation of the model. This will prepare the TF internals for training. This can simply be done with the `compile` function of the ['Model'](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) class.\n",
    "\n",
    "This function required two parameters, the loss function and an optimizer. We have one scalar value as output, and we will get one scalar value as target, so we will use the \"mean squared error\" as loss function, shorthanded to simply `'mse'`. And we will use the [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer, which is the de-facto standard optimizer for machine learning. It basically extends the *gradient descent* method with momentum, so the gradients don't change too quickly, making training more smoothly. Except for the learning rate we can use the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_training_model(model_train, learning_rate):\n",
    "    ### START CODE ###\n",
    "    # Compile the model by calling the `compile` function.\n",
    "    # Use an instance of the Adam class with the given learning rate as `optimizer`.\n",
    "    # Use the mean squared error `loss` function.\n",
    "    ...\n",
    "    \n",
    "    ### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Now let's see if you implemented all functions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = build_prediction_model()\n",
    "model_train = build_training_model(model_predict)\n",
    "compile_training_model(model_train, 1e-3)\n",
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be similar to:\n",
    "\n",
    "    Model: \"lunar-dqn-train\"\n",
    "    __________________________________________________________________________________________________\n",
    "    Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "    ==================================================================================================\n",
    "    observation (InputLayer)        [(None, 8)]          0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    lunar-dqn (Functional)          (None, 4)            43716       observation[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    action (InputLayer)             [(None, 4)]          0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    action_value (Lambda)           (None, 1)            0           lunar-dqn[0][0]                  \n",
    "                                                                     action[0][0]                     \n",
    "    ==================================================================================================\n",
    "    Total params: 43,716\n",
    "    Trainable params: 43,716\n",
    "    Non-trainable params: 0\n",
    "    __________________________________________________________________________________________________\n",
    "\n",
    "You can see that the prediction model is used as layer and has all its parameters included in the list of trainable parameters. This training model actually doesn't add any learnable layers, to it doesn't add any trainable parameters.\n",
    "\n",
    "You can also see that the output of this model is no longer a 4 column vector, but only a single value, the action value of the selected action.\n",
    "\n",
    "Let's see if it works, by predicting a single example. (Simply run this cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and make a batch of the initial observation\n",
    "observation = env.reset()\n",
    "observation = np.expand_dims(observation, axis=0)\n",
    "\n",
    "# Predict the action values with the prediction model\n",
    "action_values = model_predict.predict_on_batch(observation)\n",
    "print(f'action values predicted by prediction model = {action_values}')\n",
    "\n",
    "# Select action 2 and make it a one-hot vector (batch)\n",
    "action = to_categorical([2], num_classes=4)\n",
    "print(f'one hot {action}')\n",
    "\n",
    "# Predict the single action value with the training model\n",
    "action_value = model_train.predict_on_batch([observation, action])\n",
    "print(f'value of action 2 predicted by training model = {action_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should note that the value predicted for action `2` is exactly the same for both models. This shows that the weights of the prediction model are used by the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy policy\n",
    "\n",
    "Like in the previous exercise we will need an $\\epsilon$-greedy policy to explore during training. This time the policy will be based on the output of the prediction model (`model_predict`) given an observation. Implement the epsilon greedy policy using [`np.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html), `env.action_space.sample()`, [`predict_on_batch`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_on_batch), and [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(env, model_predict, observation, epsilon):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Implement epsilon greedy algorithm.\n",
    "    # With probability `epsilon` take a random action,\n",
    "    # otherwise take the action with the highest probability.\n",
    "    ...\n",
    "        \n",
    "    ### END CODE ###\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Training of the model will be done on batches of experience (i.e. actor-environment interactions). These batches constain:\n",
    "\n",
    "- `observation`: The (current) observation, i.e. $S_t$\n",
    "- `action`: The action taken, i.e. $A_t$\n",
    "- `reward`: The reward returned by the environment after performing the action, i.e. $R_{t+1}$\n",
    "- `next_observation`: The next observation after the action is performed, i.e. $S_{t+1}$\n",
    "- `done`: Wheter or not the episode if finished. In other words, if the `next_observation` is of a terminal state.\n",
    "\n",
    "From such a batch we have to compute the target values for the neural network. This means we have to compute $Q(S_t,A_t)$. Like before, we will be using the the TD(0) algorithm, so our target will be bootstrapped with the prediction of the next observation.\n",
    "\n",
    "$$ R_t + \\gamma \\max_a Q(S_{t+1}, a) $$\n",
    "\n",
    "Implement that in the following function. You have to use [`predict_on_batch`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_on_batch) to get the action values for the next observation. Then use [`np.amax`](https://numpy.org/doc/stable/reference/generated/numpy.amax.html) to get the value of the action with the highest value.\n",
    "\n",
    "The value of terminal states is always 0 (remember $G_T = 0$), so $Q(S_T, a)$ must be 0 as well. You can use the `dones` array to correctly filter out those values. `dones` is an array of booleans, but similar to the C programming language you can use them as integers. A `True` value is processed as a `1` and a `False` value is processed as `0`. So, if you multiply the maximum action value (`max_next_q`) with `(1-dones)` you will end up with a valid maximum action value in samples that do not end in a terminal state, otherwise the result is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_values(model_predict, rewards, next_observations, dones, gamma):\n",
    "    ### START CODE ###\n",
    "    \n",
    "    # Estimate the action values for the next observation using the prediction model\n",
    "    next_q = ...\n",
    "    \n",
    "    # Find the maximum action value (use `axis` 1)\n",
    "    # Make sure to set `keepdims=True` so the number of dimensions of the array don't change.\n",
    "    max_next_q = ...\n",
    "    \n",
    "    # Compute the target action value as described above.\n",
    "    # Take care of the terminal state\n",
    "    target_q = ...\n",
    "    \n",
    "    ### END CODE ###\n",
    "    return target_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test this. Run the following cell and see if your implementation is correct. A set of files are used to make sure the situation is always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = build_prediction_model()\n",
    "model_predict.load_weights('data/lunar_test_model.h5')\n",
    "\n",
    "r = np.load('data/lunar_test_rewards.npy')\n",
    "o = np.load('data/lunar_test_next_observations.npy')\n",
    "d = np.load('data/lunar_test_dones.npy')\n",
    "compute_target_values(model_predict, r, o, d, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this test should be exactly equal to:\n",
    "\n",
    "    array([[  61.207016],\n",
    "           [ -26.601854],\n",
    "           [-100.      ]], dtype=float32)\n",
    "\n",
    "Note that it's shape is `(3,1)`, i.e. a single value for each given sample.\n",
    "If your output is not the same, then check your implementation! Make sure the shapes of the intermediate steps are correct. Check this using for instance `print(max_next_q.shape)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step\n",
    "\n",
    "We now have nearly all the pieces in place to start training. The last missing piece is the training step itself.\n",
    "\n",
    "TensorFlow models have a very simple way of training, you can simply call the function [`fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) with the inputs for the model and a target output and it will incrementally change the weights of the model so the output fits the given target values. It will do the following steps automatically.\n",
    "- split the input data into smaller mini-batches,\n",
    "- compute the output of the network for the given input data (i.e. predict),\n",
    "- compute the loss of the output using the given target values,\n",
    "- compute the gradients of the network weights with respect to the loss,\n",
    "- update the weights of the network using the selected optimizer\n",
    "\n",
    "Performing one such sequence is called one epoch, and the `fit` function will do a number of epochs. For our RL problem we only want to run one epoch and then gather new data with an updated model.\n",
    "\n",
    "Implement the next function. It receives both models as parameters, the predict and train models, the predict model is needed to compute the target values (using `compute_target_values`) and the train model is used to actually train. Furthermore it gets the a batch of data (`observations`, `actions`, `rewards`, `next_observations`, `dones`) and the hyperparameter `gamma` ($\\gamma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_predict, model_train, observations, actions, rewards, next_observations, dones, gamma):\n",
    "    ### START CODE ###\n",
    "\n",
    "    # Compute target values\n",
    "    target_q = ...\n",
    "\n",
    "    # Fit the train model to this data.\n",
    "    # It needs both the `observations` and `actions` as parameter 'x' (use an array [])\n",
    "    # and it needs `target_q` as parameter 'y'.\n",
    "    # Run for only 1 epoch, and turn off output with verbose=0.\n",
    "    ...\n",
    "    \n",
    "    ### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Finally, we can implement the training loop. Let's define a number of hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "epsilon = 0.3\n",
    "gamma = 0.99\n",
    "batch_size = 256\n",
    "buffer_size = 20000\n",
    "nr_episodes = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the prediction and training models, so we can start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = build_prediction_model()\n",
    "model_train = build_training_model(model_predict)\n",
    "compile_training_model(model_train, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done while performing actions, but the training data itself consists of random samples for earlier gathered experience. We will use an experience buffer for that which is already implemented in the [utils.py](utils.py) file. If you want you can take a look how it works. It is basically a big queue of data with a maximum capacity. When new data is added the oldest data is removed/overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_buffer = ExperienceBuffer(buffer_size, (8,), (4,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop basically consists of three steps.\n",
    "1. Perform an action by using the $\\epsilon$-greedy policy.\n",
    "2. Store experience in a buffer.\n",
    "3. Take a random batch from the experience buffer and train on it.\n",
    "\n",
    "When an episode is finished we start a new episode and continue with these steps.\n",
    "Fill in the required steps below and run it. It will show you the progess it makes per episode. You should see the scores go up on average.\n",
    "\n",
    "**Running this training loop can take up to 15 minutes.** Take a coffee/tea break and come back later to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "for episode in range(nr_episodes):\n",
    "    episode_length, episode_score = 0, 0\n",
    "\n",
    "    observation, done = env.reset(), False\n",
    "    while not done:\n",
    "        ### START CODE ###\n",
    "        # Select an action using the epsilon-greedy policy\n",
    "        action = ...\n",
    "        \n",
    "        # Take action and get reward\n",
    "        next_observation, reward, done, _ = ...\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Convert action to one-hot vector\n",
    "        selected_action = to_categorical(action, num_classes=4)\n",
    "\n",
    "        # Add to experience buffer\n",
    "        experience_buffer.append(observation, selected_action, reward, next_observation, done)\n",
    "        if experience_buffer.is_filled(batch_size):\n",
    "            # Select a random batch from experience\n",
    "            observations, actions, rewards, next_observations, dones = experience_buffer.sample(batch_size)\n",
    "\n",
    "            ### START CODE ###\n",
    "            # Perform one train step on the batch\n",
    "            ...\n",
    "            ### END CODE ###\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        observation = next_observation\n",
    "        episode_length += 1\n",
    "        episode_score += reward\n",
    "\n",
    "    # Show statistics\n",
    "    t = (datetime.now() - start_time).total_seconds()\n",
    "    print(f'[{t:.1f}] #{episode}: length {episode_length}, score {episode_score:.0f}')\n",
    "\n",
    "# Save the end result\n",
    "model_predict.save('lunar.h5')\n",
    "\n",
    "t = (datetime.now() - start_time).total_seconds()\n",
    "print(f'Finished in {t:.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pfew, it finally finished training. By now the model should have been trained for 150 episodes and the score should on average have been increasing. Time to evaluate\n",
    "\n",
    "If it didn't increase at all, then you are one of the unlucky few that had a model converged on a local (sub-)optimum. This sometimes happen in deep reinforcment learning, and in particular in unstable environments like this. There are ways to prevent this, like choosing less agressive learning rates and bigger batch sizes, but those will increase training time a lot. For now, the only thing you can do is to restart the entire notebook. If you have the time (maybe while listening to the next part of the lectures) you can click on `Kernel`->`Restart & Run All` and try again. But first read on to the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "It took quite some time to train and that is only for a relatively small number of episodes. Let's see how well it performs anyway. We will use the following helper function to evaluate a model by taking greedy actions only (no more exploring, only exploiting) and reporting the final score. In the meantime we'll render the environment to show you what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_frame, update_frame\n",
    "\n",
    "def evaluate(model, env):\n",
    "    # Reset environment\n",
    "    obs, done, score = env.reset(), False, 0\n",
    "    frame = create_frame(env)\n",
    "\n",
    "    while not done:\n",
    "        # Predict action values\n",
    "        q = model.predict_on_batch(np.expand_dims(obs, axis=0))[0]\n",
    "        # Select action with highest value\n",
    "        action = np.argmax(q)\n",
    "\n",
    "        # Perform action\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "        update_frame(frame)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see how the randomly initialized model performs. It should be really bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(build_prediction_model(), env)\n",
    "print(f'initial model: {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well your trained model performs. We'll load the model as saved at the end of your training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "score = evaluate(load_model('lunar.h5'), env)\n",
    "print(f'partially trained model: {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should perform a lot better. It should no longer instantly crash and burn. But it performs nowhere near optimal. For that we need to train it a lot longer. Here is the performance of a model that was trained for 500 episodes with a learning rate of 3e-5 and then another 500 episodes with learning rate 1e-5. This took about 80 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(load_model('trained/lunar.h5'), env)\n",
    "print(f'fully trained model: {score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Pfew, that's it. We have trained a model-free agent. Purely by interacting with the environment it learned what actions are best for each observation it receives. It takes quite some time to train, and it didn't event got that far. It requires a lot more time and experience to train to a decent score. But, the size of the observation is also a lot bigger than before. There are a number of things you can do to improve training time and sample efficieny (how well does it learn for a single interaction with the environment), but that's out-of-scope for this training.\n",
    "\n",
    "In the next and final exercise we will see that you can skip a few steps and make training a lot easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
